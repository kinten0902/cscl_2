{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import MeCab\n",
    "import pickle\n",
    "import numpy as np\n",
    "import data_helpers as dh\n",
    "import pandas as pd\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import MDS,TSNE\n",
    "from collections import Counter\n",
    "from collections import OrderedDict\n",
    "from gensim.models import word2vec\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sen</th>\n",
       "      <th>sen_pre</th>\n",
       "      <th>say_id</th>\n",
       "      <th>reply_id</th>\n",
       "      <th>group_id</th>\n",
       "      <th>name</th>\n",
       "      <th>body</th>\n",
       "      <th>16types_a</th>\n",
       "      <th>16types_b</th>\n",
       "      <th>argument_a</th>\n",
       "      <th>argument_b</th>\n",
       "      <th>epistemic_a</th>\n",
       "      <th>epistemic_b</th>\n",
       "      <th>social_a</th>\n",
       "      <th>social_b</th>\n",
       "      <th>coordination_a</th>\n",
       "      <th>coordination_b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[よろしく, お願い, し, ます, ！, EOS]</td>\n",
       "      <td>[EOS]</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1234568</td>\n",
       "      <td>まこぴす</td>\n",
       "      <td>よろしくお願いします！</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[よろしく, お願い, し, ます, EOS]</td>\n",
       "      <td>[よろしく, お願い, し, ます, ！, EOS]</td>\n",
       "      <td>31</td>\n",
       "      <td>-1</td>\n",
       "      <td>1234568</td>\n",
       "      <td>哲</td>\n",
       "      <td>よろしくお願いします</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[名前, な, の, が, 恥ずかしい, です, EOS, よろしく, お願い, し, ます...</td>\n",
       "      <td>[よろしく, お願い, し, ます, EOS]</td>\n",
       "      <td>70</td>\n",
       "      <td>-1</td>\n",
       "      <td>1234568</td>\n",
       "      <td>仙波</td>\n",
       "      <td>名前なのが恥ずかしいです…\\nよろしくお願いします！</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 sen  \\\n",
       "0                         [よろしく, お願い, し, ます, ！, EOS]   \n",
       "1                            [よろしく, お願い, し, ます, EOS]   \n",
       "2  [名前, な, の, が, 恥ずかしい, です, EOS, よろしく, お願い, し, ます...   \n",
       "\n",
       "                      sen_pre say_id reply_id group_id  name  \\\n",
       "0                       [EOS]      1       -1  1234568  まこぴす   \n",
       "1  [よろしく, お願い, し, ます, ！, EOS]     31       -1  1234568     哲   \n",
       "2     [よろしく, お願い, し, ます, EOS]     70       -1  1234568    仙波   \n",
       "\n",
       "                         body 16types_a 16types_b argument_a argument_b  \\\n",
       "0                 よろしくお願いします！         5         5          1          1   \n",
       "1                  よろしくお願いします         5         5          1          1   \n",
       "2  名前なのが恥ずかしいです…\\nよろしくお願いします！         5         5          1          1   \n",
       "\n",
       "  epistemic_a epistemic_b social_a social_b coordination_a coordination_b  \n",
       "0           1           1        0        0              0              0  \n",
       "1           1           1        0        0              0              0  \n",
       "2           1           1        0        0              0              0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "All_df = pd.read_pickle(\"../data/all_mecab.pickle\")\n",
    "All_df.head(3)\n",
    "\n",
    "# senとsen_preの単語をIDに変換し、新たな列としてAll_dfに追加する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# senとsen_preの単語をIDに変換し、新たな列としてAll_dfに追加する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "単語: EOS \t出現数: 29580 \tID: 0\n",
      "単語: の \t出現数: 10567 \tID: 1\n",
      "単語: て \t出現数: 7408 \tID: 2\n",
      "単語: です \t出現数: 7390 \tID: 3\n",
      "単語: ます \t出現数: 7363 \tID: 4\n",
      "単語: か \t出現数: 7285 \tID: 5\n",
      "単語: 、 \t出現数: 6959 \tID: 6\n",
      "単語: に \t出現数: 6914 \tID: 7\n",
      "単語: が \t出現数: 6893 \tID: 8\n",
      "単語: は \t出現数: 6793 \tID: 9\n",
      "words kinds: 6961 words>=2: 6649\n"
     ]
    }
   ],
   "source": [
    "sen = All_df['sen'].values\n",
    "sen_pre = All_df['sen_pre'].values\n",
    "\n",
    "# 単語辞書の作成\n",
    "wd_set = Counter([x for s in (sen + sen_pre) for x in s])\n",
    "wd_ary = np.array(list(wd_set.keys()))\n",
    "wd_cnt = np.array(list(wd_set.values()))\n",
    "\n",
    "# 出現頻度順にソート\n",
    "wd_ary = wd_ary[np.argsort(wd_cnt)[::-1]]\n",
    "wd_cnt.sort()\n",
    "wd_cnt = wd_cnt[::-1]\n",
    "\n",
    "# 単語ID辞書の作成\n",
    "wd_to_id = {wd: i for i, wd in enumerate(wd_ary)}\n",
    "\n",
    "# Top10の単語を出力\n",
    "for i in range(10):\n",
    "    print(\"単語:\",\n",
    "          list(wd_ary)[i], \"\\t出現数:\",\n",
    "          list(wd_cnt)[i], \"\\tID:\", wd_to_id[list(wd_ary)[i]])\n",
    "\n",
    "# 出現数CUT_OFF以下の単語のIDを統一\n",
    "CUT_OFF = 2\n",
    "print(\"words kinds:\", len(wd_cnt), \"words>=\" + str(CUT_OFF) + \":\",\n",
    "      np.sum(wd_cnt >= CUT_OFF))\n",
    "other_id = np.sum(wd_cnt >= CUT_OFF)\n",
    "wd_to_id.update({wd: other_id for wd in wd_ary[wd_cnt < CUT_OFF]})\n",
    "id_to_wd = {wd_to_id[wd]: wd for wd in wd_to_id.keys()}\n",
    "\n",
    "# senとsen_preの単語をIDに変換\n",
    "sen_id = []\n",
    "sen_pre_id = []\n",
    "for s, s_pre in zip(sen, sen_pre):\n",
    "    sen_id.append([str(wd_to_id[wd]) for wd in s])\n",
    "    sen_pre_id.append([str(wd_to_id[wd]) for wd in s_pre])\n",
    "\n",
    "# 新し列としてAll_dfに追加\n",
    "All_df.insert(loc=0, column='sen_id', value=sen_id)\n",
    "All_df.insert(loc=1, column='sen_pre_id', value=sen_pre_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sen_id</th>\n",
       "      <th>sen_pre_id</th>\n",
       "      <th>sen</th>\n",
       "      <th>sen_pre</th>\n",
       "      <th>say_id</th>\n",
       "      <th>reply_id</th>\n",
       "      <th>group_id</th>\n",
       "      <th>name</th>\n",
       "      <th>body</th>\n",
       "      <th>16types_a</th>\n",
       "      <th>16types_b</th>\n",
       "      <th>argument_a</th>\n",
       "      <th>argument_b</th>\n",
       "      <th>epistemic_a</th>\n",
       "      <th>epistemic_b</th>\n",
       "      <th>social_a</th>\n",
       "      <th>social_b</th>\n",
       "      <th>coordination_a</th>\n",
       "      <th>coordination_b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[39, 35, 12, 4, 18, 0]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[よろしく, お願い, し, ます, ！, EOS]</td>\n",
       "      <td>[EOS]</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1234568</td>\n",
       "      <td>まこぴす</td>\n",
       "      <td>よろしくお願いします！</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[39, 35, 12, 4, 0]</td>\n",
       "      <td>[39, 35, 12, 4, 18, 0]</td>\n",
       "      <td>[よろしく, お願い, し, ます, EOS]</td>\n",
       "      <td>[よろしく, お願い, し, ます, ！, EOS]</td>\n",
       "      <td>31</td>\n",
       "      <td>-1</td>\n",
       "      <td>1234568</td>\n",
       "      <td>哲</td>\n",
       "      <td>よろしくお願いします</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[415, 19, 1, 8, 2222, 3, 0, 39, 35, 12, 4, 18, 0]</td>\n",
       "      <td>[39, 35, 12, 4, 0]</td>\n",
       "      <td>[名前, な, の, が, 恥ずかしい, です, EOS, よろしく, お願い, し, ます...</td>\n",
       "      <td>[よろしく, お願い, し, ます, EOS]</td>\n",
       "      <td>70</td>\n",
       "      <td>-1</td>\n",
       "      <td>1234568</td>\n",
       "      <td>仙波</td>\n",
       "      <td>名前なのが恥ずかしいです…\\nよろしくお願いします！</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sen_id              sen_pre_id  \\\n",
       "0                             [39, 35, 12, 4, 18, 0]                     [0]   \n",
       "1                                 [39, 35, 12, 4, 0]  [39, 35, 12, 4, 18, 0]   \n",
       "2  [415, 19, 1, 8, 2222, 3, 0, 39, 35, 12, 4, 18, 0]      [39, 35, 12, 4, 0]   \n",
       "\n",
       "                                                 sen  \\\n",
       "0                         [よろしく, お願い, し, ます, ！, EOS]   \n",
       "1                            [よろしく, お願い, し, ます, EOS]   \n",
       "2  [名前, な, の, が, 恥ずかしい, です, EOS, よろしく, お願い, し, ます...   \n",
       "\n",
       "                      sen_pre say_id reply_id group_id  name  \\\n",
       "0                       [EOS]      1       -1  1234568  まこぴす   \n",
       "1  [よろしく, お願い, し, ます, ！, EOS]     31       -1  1234568     哲   \n",
       "2     [よろしく, お願い, し, ます, EOS]     70       -1  1234568    仙波   \n",
       "\n",
       "                         body 16types_a 16types_b argument_a argument_b  \\\n",
       "0                 よろしくお願いします！         5         5          1          1   \n",
       "1                  よろしくお願いします         5         5          1          1   \n",
       "2  名前なのが恥ずかしいです…\\nよろしくお願いします！         5         5          1          1   \n",
       "\n",
       "  epistemic_a epistemic_b social_a social_b coordination_a coordination_b  \n",
       "0           1           1        0        0              0              0  \n",
       "1           1           1        0        0              0              0  \n",
       "2           1           1        0        0              0              0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "All_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec を利用し、単語のベクトル辞書を作成する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_sen_length: 292\n"
     ]
    }
   ],
   "source": [
    "sen_id = All_df['sen_id'].values\n",
    "sen_pre_id = All_df['sen_pre_id'].values\n",
    "sen_all = np.hstack((sen_id, sen_pre_id))\n",
    "\n",
    "max_sen_length = max([len(sen) for sen in sen_all])\n",
    "print(\"max_sen_length:\", max_sen_length)\n",
    "\n",
    "word_vectors_size = 200\n",
    "\n",
    "model = dh.get_w2v_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# （重要）各センテンスの長さを66に統一する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_sen_length: 66\n"
     ]
    }
   ],
   "source": [
    "All_df['sen_id'] = [x[:66] for x in All_df['sen_id']]\n",
    "All_df['sen_pre_id'] = [x[:66] for x in All_df['sen_pre_id']]\n",
    "\n",
    "sen_all = np.hstack((All_df['sen_id'].values, All_df['sen_pre_id'].values))\n",
    "max_sen_length = max([len(sen) for sen in sen_all])\n",
    "print(\"max_sen_length:\", max_sen_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データの準備\n",
    "* sentences dataをpaddingし、word vectorsによりfeature vectorsを作る\n",
    "* labels dataをone hotの型に変換する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "全データ(All_df)： (12012, 19)\n",
      "重複投稿を排除したデータ(All_drop_df)： (11357, 19) \n",
      "\n",
      "input data(sen)： (11357, 66, 200)\n",
      "input data(sen_pre)： (11357, 66, 200)\n"
     ]
    }
   ],
   "source": [
    "# データの整理（一致、重複）\n",
    "print(\"全データ(All_df)：\", All_df.shape)\n",
    "All_drop_df = All_df.drop_duplicates(subset=['body', 'name']).reset_index(drop=True)\n",
    "print(\"重複投稿を排除したデータ(All_drop_df)：\", All_drop_df.shape, \"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "all_sen = All_drop_df['sen_id'].values\n",
    "all_sen = np.array([np.array(x, dtype=np.int32) for x in all_sen])\n",
    "x = dh.sen_to_fv(all_sen, max_sen_length, model, False)\n",
    "print(\"input data(sen)：\",x.shape)\n",
    "all_sen_pre = All_drop_df['sen_pre_id'].values\n",
    "all_sen_pre = np.array([np.array(x, dtype=np.int32) for x in all_sen_pre])\n",
    "x_pre = dh.sen_to_fv(all_sen_pre, max_sen_length, model, False)\n",
    "print(\"input data(sen_pre)：\",x_pre.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16types_a： Counter({1: 2425, 2: 1938, 6: 1307, 3: 1301, 4: 1230, 5: 1224, 8: 550, 9: 510, 7: 393, 14: 310, 15: 218, 11: 184, 0: 143, 10: 123, 12: 100, 13: 56}) \n",
      "\n",
      "argument_a： Counter({1: 5804, 2: 3204, 0: 2352, 3: 509, 4: 120, 5: 23}) \n",
      "\n",
      "input data(t16_a)： (11357, 16)\n",
      "input data(arg_a)： (11357, 6)\n",
      "(1136, 16)\n",
      "(1136, 6)\n"
     ]
    }
   ],
   "source": [
    "# 16types-------------------------------------\n",
    "print(\"16types_a：\", Counter(All_df['16types_a']), \"\\n\")\n",
    "print(\"argument_a：\", Counter(All_df['argument_a']), \"\\n\")\n",
    "\n",
    "label_t16_a = All_drop_df['16types_a'].values\n",
    "label_t16_a = np.array(label_t16_a, dtype=np.int32)\n",
    "y_t16_a = dh.labels_to_one_hot(label_t16_a, 16)\n",
    "print(\"input data(t16_a)：\", y_t16_a.shape)\n",
    "\n",
    "label_arg_a = All_drop_df['argument_a'].values\n",
    "label_arg_a = np.array(label_arg_a, dtype=np.int32)\n",
    "y_arg_a = dh.labels_to_one_hot(label_arg_a, 6)\n",
    "print(\"input data(arg_a)：\", y_arg_a.shape)\n",
    "\n",
    "data = dh.set_data_sets_2(x, x_pre, y_t16_a, y_arg_a)\n",
    "\n",
    "\n",
    "print(data.test.labels_1.shape)\n",
    "print(data.test.labels_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159\n"
     ]
    }
   ],
   "source": [
    "# Network Parameters\n",
    "num_input = 200\n",
    "num_hidden = 200\n",
    "num_classes_1 = 16\n",
    "num_classes_2 = 6\n",
    "train_dropout = 1.0\n",
    "test_dropout = 1.0\n",
    "embed_dim = word_vectors_size\n",
    "sents_len = max_sen_length\n",
    "\n",
    "\n",
    "# Training Parameters\n",
    "learning_rate = 0.01\n",
    "batch_size = 64\n",
    "total_batch = int(data.train.num_examples / batch_size)\n",
    "print(total_batch)\n",
    "training_epochs = 100\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(dtype=tf.float32, shape=[None, sents_len, embed_dim])\n",
    "Y_1 = tf.placeholder(dtype=tf.float32, shape=[None, num_classes_1])\n",
    "Y_2 = tf.placeholder(dtype=tf.float32, shape=[None, num_classes_2])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.truncated_normal(stddev=0.1, shape=[num_input, num_hidden])),\n",
    "    'out_1': tf.Variable(tf.truncated_normal(stddev=0.1, shape=[num_hidden, num_classes_1])),\n",
    "    'out_2': tf.Variable(tf.truncated_normal(stddev=0.1, shape=[num_hidden, num_classes_2]))\n",
    "}\n",
    "biases = {\n",
    "    'h1': tf.Variable(tf.constant(value=0.1, shape=[num_hidden])),\n",
    "    'out_1': tf.Variable(tf.constant(value=0.1, shape=[num_classes_1])),\n",
    "    'out_2': tf.Variable(tf.constant(value=0.1, shape=[num_classes_2]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "def NN(x, weights, biases, dropout):\n",
    "\n",
    "    avg = tf.reduce_mean(x, axis=1) # [None, embed_dim]\n",
    "\n",
    "    h1 = tf.add(tf.matmul(avg, weights['h1']), biases['h1'])\n",
    "    h1_relu = tf.nn.relu(h1)\n",
    "    \n",
    "    h1_drop = tf.nn.dropout(h1_relu, dropout)\n",
    "    \n",
    "    out_1 = tf.add(tf.matmul(h1_drop, weights['out_1']), biases['out_1'])\n",
    "    out_2 = tf.add(tf.matmul(h1_drop, weights['out_2']), biases['out_2'])\n",
    "    return out_1, out_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct model\n",
    "y_pred_1, y_pred_2 = NN(X, weights, biases, keep_prob)\n",
    "\n",
    "# y_softmax = tf.nn.softmax(y_pred)\n",
    "\n",
    "# Define loss and optimizer\n",
    "# type 1(old):\n",
    "# loss = tf.reduce_mean(\n",
    "#     -tf.reduce_sum(Y * tf.log(y_softmax), reduction_indices=[1]))\n",
    "# type 2(server):\n",
    "# loss = tf.reduce_mean(\n",
    "#     tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=y_pred))\n",
    "# type 3(new):\n",
    "# loss = tf.reduce_mean(\n",
    "#     tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=y_pred))\n",
    "\n",
    "\n",
    "loss_1 = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=Y_1, logits=y_pred_1))\n",
    "\n",
    "loss_2 = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=Y_2, logits=y_pred_2))\n",
    "\n",
    "loss_joint = loss_1 + loss_2\n",
    "\n",
    "optimizer_1 = tf.train.AdamOptimizer(learning_rate).minimize(loss_1)\n",
    "optimizer_2 = tf.train.AdamOptimizer(learning_rate).minimize(loss_2)\n",
    "\n",
    "optimizer_joint = tf.train.AdamOptimizer(learning_rate).minimize(loss_joint)\n",
    "\n",
    "\n",
    "# Evaluate model\n",
    "pred_1 = tf.argmax(y_pred_1, 1)\n",
    "true_1 = tf.argmax(Y_1, 1)\n",
    "correct_prediction_1 = tf.equal(pred_1, true_1)\n",
    "accuracy_1 = tf.reduce_mean(tf.cast(correct_prediction_1, tf.float32))\n",
    "\n",
    "\n",
    "pred_2 = tf.argmax(y_pred_2, 1)\n",
    "true_2 = tf.argmax(Y_2, 1)\n",
    "correct_prediction_2 = tf.equal(pred_2, true_2)\n",
    "accuracy_2 = tf.reduce_mean(tf.cast(correct_prediction_2, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep:  1 Bc: 159 | train_1=0.234 test_1=0.158 | train_2=1.000 test_2=0.199 | loss_joint=3.624\n",
      "Ep:  2 Bc: 159 | train_1=0.312 test_1=0.298 | train_2=0.469 test_2=0.523 | loss_joint=3.499\n",
      "Ep:  3 Bc: 159 | train_1=0.297 test_1=0.298 | train_2=0.438 test_2=0.541 | loss_joint=3.238\n",
      "Ep:  4 Bc: 159 | train_1=0.391 test_1=0.317 | train_2=0.453 test_2=0.518 | loss_joint=3.196\n",
      "Ep:  5 Bc: 159 | train_1=0.344 test_1=0.304 | train_2=0.594 test_2=0.544 | loss_joint=3.158\n",
      "Ep:  6 Bc: 159 | train_1=0.281 test_1=0.325 | train_2=0.531 test_2=0.546 | loss_joint=3.141\n",
      "Ep:  7 Bc: 159 | train_1=0.375 test_1=0.347 | train_2=0.609 test_2=0.553 | loss_joint=3.081\n",
      "Ep:  8 Bc: 159 | train_1=0.469 test_1=0.353 | train_2=0.469 test_2=0.561 | loss_joint=3.049\n",
      "Ep:  9 Bc: 159 | train_1=0.328 test_1=0.359 | train_2=0.562 test_2=0.559 | loss_joint=3.012\n",
      "Ep: 10 Bc: 159 | train_1=0.391 test_1=0.377 | train_2=0.609 test_2=0.562 | loss_joint=3.000\n",
      "Ep: 11 Bc: 159 | train_1=0.391 test_1=0.371 | train_2=0.516 test_2=0.567 | loss_joint=2.961\n",
      "Ep: 12 Bc: 159 | train_1=0.406 test_1=0.363 | train_2=0.609 test_2=0.581 | loss_joint=2.934\n",
      "Ep: 13 Bc: 159 | train_1=0.281 test_1=0.364 | train_2=0.609 test_2=0.574 | loss_joint=2.915\n",
      "Ep: 14 Bc: 159 | train_1=0.422 test_1=0.369 | train_2=0.547 test_2=0.570 | loss_joint=2.902\n",
      "Ep: 15 Bc: 159 | train_1=0.469 test_1=0.379 | train_2=0.484 test_2=0.581 | loss_joint=2.903\n",
      "Ep: 16 Bc: 159 | train_1=0.438 test_1=0.378 | train_2=0.578 test_2=0.562 | loss_joint=2.900\n",
      "Ep: 17 Bc: 159 | train_1=0.406 test_1=0.361 | train_2=0.625 test_2=0.577 | loss_joint=2.888\n",
      "Ep: 18 Bc: 159 | train_1=0.391 test_1=0.393 | train_2=0.562 test_2=0.555 | loss_joint=2.882\n",
      "Ep: 19 Bc: 159 | train_1=0.297 test_1=0.393 | train_2=0.500 test_2=0.567 | loss_joint=2.864\n",
      "Ep: 20 Bc: 159 | train_1=0.453 test_1=0.392 | train_2=0.625 test_2=0.578 | loss_joint=2.868\n",
      "Ep: 21 Bc: 159 | train_1=0.375 test_1=0.363 | train_2=0.625 test_2=0.579 | loss_joint=2.870\n",
      "Ep: 22 Bc: 159 | train_1=0.281 test_1=0.371 | train_2=0.547 test_2=0.580 | loss_joint=2.855\n",
      "Ep: 23 Bc: 159 | train_1=0.453 test_1=0.408 | train_2=0.578 test_2=0.563 | loss_joint=2.855\n",
      "Ep: 24 Bc: 159 | train_1=0.406 test_1=0.411 | train_2=0.734 test_2=0.572 | loss_joint=2.848\n",
      "Ep: 25 Bc: 159 | train_1=0.406 test_1=0.391 | train_2=0.484 test_2=0.560 | loss_joint=2.847\n",
      "Ep: 26 Bc: 159 | train_1=0.453 test_1=0.407 | train_2=0.469 test_2=0.574 | loss_joint=2.853\n",
      "Ep: 27 Bc: 159 | train_1=0.484 test_1=0.349 | train_2=0.609 test_2=0.573 | loss_joint=2.846\n",
      "Ep: 28 Bc: 159 | train_1=0.344 test_1=0.390 | train_2=0.578 test_2=0.579 | loss_joint=2.830\n",
      "Ep: 29 Bc: 159 | train_1=0.453 test_1=0.419 | train_2=0.609 test_2=0.559 | loss_joint=2.835\n",
      "Ep: 30 Bc: 159 | train_1=0.375 test_1=0.401 | train_2=0.578 test_2=0.583 | loss_joint=2.834\n",
      "Ep: 31 Bc: 159 | train_1=0.516 test_1=0.408 | train_2=0.688 test_2=0.576 | loss_joint=2.830\n",
      "Ep: 32 Bc: 159 | train_1=0.438 test_1=0.391 | train_2=0.547 test_2=0.584 | loss_joint=2.827\n",
      "Ep: 33 Bc: 159 | train_1=0.375 test_1=0.401 | train_2=0.609 test_2=0.584 | loss_joint=2.829\n",
      "Ep: 34 Bc: 159 | train_1=0.344 test_1=0.415 | train_2=0.594 test_2=0.561 | loss_joint=2.834\n",
      "Ep: 35 Bc: 159 | train_1=0.375 test_1=0.421 | train_2=0.547 test_2=0.560 | loss_joint=2.813\n",
      "Ep: 36 Bc: 159 | train_1=0.469 test_1=0.409 | train_2=0.656 test_2=0.566 | loss_joint=2.830\n",
      "Ep: 37 Bc: 159 | train_1=0.500 test_1=0.408 | train_2=0.562 test_2=0.581 | loss_joint=2.823\n",
      "Ep: 38 Bc: 159 | train_1=0.422 test_1=0.402 | train_2=0.438 test_2=0.584 | loss_joint=2.831\n",
      "Ep: 39 Bc: 159 | train_1=0.531 test_1=0.411 | train_2=0.578 test_2=0.561 | loss_joint=2.823\n",
      "Ep: 40 Bc: 159 | train_1=0.516 test_1=0.406 | train_2=0.547 test_2=0.577 | loss_joint=2.812\n",
      "Ep: 41 Bc: 159 | train_1=0.375 test_1=0.396 | train_2=0.531 test_2=0.564 | loss_joint=2.810\n",
      "Ep: 42 Bc: 159 | train_1=0.469 test_1=0.415 | train_2=0.484 test_2=0.567 | loss_joint=2.817\n",
      "Ep: 43 Bc: 159 | train_1=0.422 test_1=0.423 | train_2=0.609 test_2=0.574 | loss_joint=2.815\n",
      "Ep: 44 Bc: 159 | train_1=0.391 test_1=0.413 | train_2=0.516 test_2=0.581 | loss_joint=2.801\n",
      "Ep: 45 Bc: 159 | train_1=0.453 test_1=0.415 | train_2=0.547 test_2=0.568 | loss_joint=2.807\n",
      "Ep: 46 Bc: 159 | train_1=0.453 test_1=0.416 | train_2=0.516 test_2=0.571 | loss_joint=2.809\n",
      "Ep: 47 Bc: 159 | train_1=0.312 test_1=0.395 | train_2=0.484 test_2=0.575 | loss_joint=2.802\n",
      "Ep: 48 Bc: 159 | train_1=0.422 test_1=0.385 | train_2=0.516 test_2=0.564 | loss_joint=2.804\n",
      "Ep: 49 Bc: 159 | train_1=0.344 test_1=0.397 | train_2=0.625 test_2=0.568 | loss_joint=2.806\n",
      "Ep: 50 Bc: 159 | train_1=0.391 test_1=0.395 | train_2=0.656 test_2=0.583 | loss_joint=2.821\n",
      "Ep: 51 Bc: 159 | train_1=0.391 test_1=0.426 | train_2=0.547 test_2=0.576 | loss_joint=2.799\n",
      "Ep: 52 Bc: 159 | train_1=0.500 test_1=0.420 | train_2=0.500 test_2=0.585 | loss_joint=2.796\n",
      "Ep: 53 Bc: 159 | train_1=0.438 test_1=0.423 | train_2=0.641 test_2=0.585 | loss_joint=2.802\n",
      "Ep: 54 Bc: 159 | train_1=0.453 test_1=0.421 | train_2=0.469 test_2=0.570 | loss_joint=2.796\n",
      "Ep: 55 Bc: 159 | train_1=0.359 test_1=0.401 | train_2=0.469 test_2=0.581 | loss_joint=2.814\n",
      "Ep: 56 Bc: 159 | train_1=0.422 test_1=0.422 | train_2=0.641 test_2=0.580 | loss_joint=2.799\n",
      "Ep: 57 Bc: 159 | train_1=0.469 test_1=0.417 | train_2=0.594 test_2=0.581 | loss_joint=2.811\n",
      "Ep: 58 Bc: 159 | train_1=0.469 test_1=0.417 | train_2=0.516 test_2=0.575 | loss_joint=2.796\n",
      "Ep: 59 Bc: 159 | train_1=0.328 test_1=0.423 | train_2=0.609 test_2=0.586 | loss_joint=2.801\n",
      "Ep: 60 Bc: 159 | train_1=0.391 test_1=0.415 | train_2=0.578 test_2=0.568 | loss_joint=2.798\n",
      "Ep: 61 Bc: 159 | train_1=0.438 test_1=0.407 | train_2=0.547 test_2=0.577 | loss_joint=2.785\n",
      "Ep: 62 Bc: 159 | train_1=0.438 test_1=0.416 | train_2=0.516 test_2=0.577 | loss_joint=2.804\n",
      "Ep: 63 Bc: 159 | train_1=0.453 test_1=0.423 | train_2=0.578 test_2=0.568 | loss_joint=2.793\n",
      "Ep: 64 Bc: 159 | train_1=0.547 test_1=0.384 | train_2=0.578 test_2=0.564 | loss_joint=2.792\n",
      "Ep: 65 Bc: 159 | train_1=0.453 test_1=0.408 | train_2=0.438 test_2=0.581 | loss_joint=2.780\n",
      "Ep: 66 Bc: 159 | train_1=0.500 test_1=0.418 | train_2=0.656 test_2=0.583 | loss_joint=2.792\n",
      "Ep: 67 Bc: 159 | train_1=0.422 test_1=0.407 | train_2=0.453 test_2=0.565 | loss_joint=2.784\n",
      "Ep: 68 Bc: 159 | train_1=0.500 test_1=0.436 | train_2=0.594 test_2=0.565 | loss_joint=2.788\n",
      "Ep: 69 Bc: 159 | train_1=0.500 test_1=0.423 | train_2=0.531 test_2=0.579 | loss_joint=2.794\n",
      "Ep: 70 Bc: 159 | train_1=0.391 test_1=0.396 | train_2=0.500 test_2=0.581 | loss_joint=2.785\n",
      "Ep: 71 Bc: 159 | train_1=0.453 test_1=0.423 | train_2=0.609 test_2=0.577 | loss_joint=2.789\n",
      "Ep: 72 Bc: 159 | train_1=0.469 test_1=0.421 | train_2=0.594 test_2=0.585 | loss_joint=2.770\n",
      "Ep: 73 Bc: 159 | train_1=0.438 test_1=0.429 | train_2=0.562 test_2=0.578 | loss_joint=2.774\n",
      "Ep: 74 Bc: 159 | train_1=0.422 test_1=0.426 | train_2=0.562 test_2=0.568 | loss_joint=2.779\n",
      "Ep: 75 Bc: 159 | train_1=0.500 test_1=0.422 | train_2=0.422 test_2=0.570 | loss_joint=2.772\n",
      "Ep: 76 Bc: 159 | train_1=0.406 test_1=0.397 | train_2=0.484 test_2=0.584 | loss_joint=2.786\n",
      "Ep: 77 Bc: 159 | train_1=0.469 test_1=0.414 | train_2=0.609 test_2=0.567 | loss_joint=2.786\n",
      "Ep: 78 Bc: 159 | train_1=0.531 test_1=0.435 | train_2=0.516 test_2=0.581 | loss_joint=2.783\n",
      "Ep: 79 Bc: 159 | train_1=0.516 test_1=0.431 | train_2=0.562 test_2=0.572 | loss_joint=2.769\n",
      "Ep: 80 Bc: 159 | train_1=0.359 test_1=0.416 | train_2=0.547 test_2=0.577 | loss_joint=2.771\n",
      "Ep: 81 Bc: 159 | train_1=0.484 test_1=0.428 | train_2=0.594 test_2=0.583 | loss_joint=2.771\n",
      "Ep: 82 Bc: 159 | train_1=0.469 test_1=0.438 | train_2=0.562 test_2=0.579 | loss_joint=2.768\n",
      "Ep: 83 Bc: 159 | train_1=0.438 test_1=0.434 | train_2=0.688 test_2=0.587 | loss_joint=2.777\n",
      "Ep: 84 Bc: 159 | train_1=0.453 test_1=0.403 | train_2=0.609 test_2=0.580 | loss_joint=2.772\n",
      "Ep: 85 Bc: 159 | train_1=0.328 test_1=0.450 | train_2=0.578 test_2=0.582 | loss_joint=2.770\n",
      "Ep: 86 Bc: 159 | train_1=0.578 test_1=0.428 | train_2=0.562 test_2=0.576 | loss_joint=2.772\n",
      "Ep: 87 Bc: 159 | train_1=0.328 test_1=0.430 | train_2=0.641 test_2=0.583 | loss_joint=2.762\n",
      "Ep: 88 Bc: 159 | train_1=0.375 test_1=0.436 | train_2=0.641 test_2=0.571 | loss_joint=2.762\n",
      "Ep: 89 Bc: 159 | train_1=0.516 test_1=0.441 | train_2=0.625 test_2=0.575 | loss_joint=2.756\n",
      "Ep: 90 Bc: 159 | train_1=0.422 test_1=0.422 | train_2=0.656 test_2=0.580 | loss_joint=2.754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 91 Bc: 159 | train_1=0.484 test_1=0.450 | train_2=0.562 test_2=0.583 | loss_joint=2.770\n",
      "Ep: 92 Bc: 159 | train_1=0.422 test_1=0.438 | train_2=0.547 test_2=0.575 | loss_joint=2.761\n",
      "Ep: 93 Bc: 159 | train_1=0.312 test_1=0.434 | train_2=0.562 test_2=0.555 | loss_joint=2.738\n",
      "Ep: 94 Bc: 159 | train_1=0.359 test_1=0.436 | train_2=0.500 test_2=0.578 | loss_joint=2.737\n",
      "Ep: 95 Bc: 159 | train_1=0.375 test_1=0.441 | train_2=0.562 test_2=0.577 | loss_joint=2.738\n",
      "Ep: 96 Bc: 159 | train_1=0.391 test_1=0.424 | train_2=0.625 test_2=0.578 | loss_joint=2.740\n",
      "Ep: 97 Bc: 159 | train_1=0.406 test_1=0.418 | train_2=0.562 test_2=0.584 | loss_joint=2.746\n",
      "Ep: 98 Bc: 159 | train_1=0.406 test_1=0.440 | train_2=0.453 test_2=0.581 | loss_joint=2.735\n",
      "Ep: 99 Bc: 159 | train_1=0.406 test_1=0.452 | train_2=0.516 test_2=0.580 | loss_joint=2.723\n",
      "Ep:100 Bc: 159 | train_1=0.359 test_1=0.435 | train_2=0.547 test_2=0.581 | loss_joint=2.730\n",
      "Process Time :42.15 s\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "# Training cycle\n",
    "all_test_x = data.test.vectors_1\n",
    "all_test_y_1 = data.test.labels_1\n",
    "all_test_y_2 = data.test.labels_2\n",
    "start = time.time()\n",
    "for epoch_i in range(training_epochs):\n",
    "    ave_cost = 0\n",
    "    for batch_i in range(total_batch):\n",
    "        batch_x, _, batch_y_1, batch_y_2 = data.train.next_batch(batch_size, 2)\n",
    "        _, c = sess.run(\n",
    "            [optimizer_joint, loss_joint],\n",
    "            feed_dict={\n",
    "                X: batch_x,\n",
    "                Y_1: batch_y_1,\n",
    "                Y_2: batch_y_2,\n",
    "                keep_prob: train_dropout\n",
    "            })\n",
    "        ave_cost += c / total_batch\n",
    "    # Display logs per epoch step\n",
    "    if epoch_i % 1 == 0:\n",
    "        train_acc_1 = sess.run(\n",
    "            accuracy_1,\n",
    "            feed_dict={\n",
    "                X: batch_x,\n",
    "                Y_1: batch_y_1,\n",
    "                keep_prob: test_dropout\n",
    "            })\n",
    "        train_acc_2 = sess.run(\n",
    "            accuracy_2,\n",
    "            feed_dict={\n",
    "                X: batch_x,\n",
    "                Y_2: batch_y_2,\n",
    "                keep_prob: test_dropout\n",
    "            })\n",
    "        test_acc_1 = sess.run(\n",
    "            accuracy_1,\n",
    "            feed_dict={\n",
    "                X: all_test_x,\n",
    "                Y_1: all_test_y_1,\n",
    "                keep_prob: test_dropout\n",
    "            })\n",
    "        test_acc_2 = sess.run(\n",
    "            accuracy_2,\n",
    "            feed_dict={\n",
    "                X: all_test_x,\n",
    "                Y_2: all_test_y_2,\n",
    "                keep_prob: test_dropout\n",
    "            })\n",
    "        print(\"Ep:%3d Bc:%4d\" % (epoch_i + 1, batch_i + 1),\n",
    "              \"| train_1=%.3f\" % train_acc_1, \"test_1=%.3f\" % test_acc_1, \n",
    "              \"| train_2=%.3f\" % train_acc_2, \"test_2=%.3f\" % test_acc_2, \"| loss_joint=%5.3f\" % ave_cost)\n",
    "end = time.time()\n",
    "print(\"Process Time :%.2f s\" % (end - start))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
