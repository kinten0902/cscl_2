{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import MeCab\n",
    "import pickle\n",
    "import numpy as np\n",
    "import data_helpers as dh\n",
    "import pandas as pd\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import MDS,TSNE\n",
    "from collections import Counter\n",
    "from collections import OrderedDict\n",
    "from gensim.models import word2vec\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sen</th>\n",
       "      <th>sen_pre</th>\n",
       "      <th>say_id</th>\n",
       "      <th>reply_id</th>\n",
       "      <th>group_id</th>\n",
       "      <th>name</th>\n",
       "      <th>body</th>\n",
       "      <th>16types_a</th>\n",
       "      <th>16types_b</th>\n",
       "      <th>argument_a</th>\n",
       "      <th>argument_b</th>\n",
       "      <th>epistemic_a</th>\n",
       "      <th>epistemic_b</th>\n",
       "      <th>social_a</th>\n",
       "      <th>social_b</th>\n",
       "      <th>coordination_a</th>\n",
       "      <th>coordination_b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[よろしく, お願い, し, ます, ！, EOS]</td>\n",
       "      <td>[EOS]</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1234568</td>\n",
       "      <td>まこぴす</td>\n",
       "      <td>よろしくお願いします！</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[よろしく, お願い, し, ます, EOS]</td>\n",
       "      <td>[よろしく, お願い, し, ます, ！, EOS]</td>\n",
       "      <td>31</td>\n",
       "      <td>-1</td>\n",
       "      <td>1234568</td>\n",
       "      <td>哲</td>\n",
       "      <td>よろしくお願いします</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[名前, な, の, が, 恥ずかしい, です, EOS, よろしく, お願い, し, ます...</td>\n",
       "      <td>[よろしく, お願い, し, ます, EOS]</td>\n",
       "      <td>70</td>\n",
       "      <td>-1</td>\n",
       "      <td>1234568</td>\n",
       "      <td>仙波</td>\n",
       "      <td>名前なのが恥ずかしいです…\\nよろしくお願いします！</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 sen  \\\n",
       "0                         [よろしく, お願い, し, ます, ！, EOS]   \n",
       "1                            [よろしく, お願い, し, ます, EOS]   \n",
       "2  [名前, な, の, が, 恥ずかしい, です, EOS, よろしく, お願い, し, ます...   \n",
       "\n",
       "                      sen_pre say_id reply_id group_id  name  \\\n",
       "0                       [EOS]      1       -1  1234568  まこぴす   \n",
       "1  [よろしく, お願い, し, ます, ！, EOS]     31       -1  1234568     哲   \n",
       "2     [よろしく, お願い, し, ます, EOS]     70       -1  1234568    仙波   \n",
       "\n",
       "                         body 16types_a 16types_b argument_a argument_b  \\\n",
       "0                 よろしくお願いします！         5         5          1          1   \n",
       "1                  よろしくお願いします         5         5          1          1   \n",
       "2  名前なのが恥ずかしいです…\\nよろしくお願いします！         5         5          1          1   \n",
       "\n",
       "  epistemic_a epistemic_b social_a social_b coordination_a coordination_b  \n",
       "0           1           1        0        0              0              0  \n",
       "1           1           1        0        0              0              0  \n",
       "2           1           1        0        0              0              0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "All_df = pd.read_pickle(\"../data/all_mecab.pickle\")\n",
    "All_df.head(3)\n",
    "\n",
    "# senとsen_preの単語をIDに変換し、新たな列としてAll_dfに追加する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# senとsen_preの単語をIDに変換し、新たな列としてAll_dfに追加する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "単語: EOS \t出現数: 29580 \tID: 0\n",
      "単語: の \t出現数: 10567 \tID: 1\n",
      "単語: て \t出現数: 7408 \tID: 2\n",
      "単語: です \t出現数: 7390 \tID: 3\n",
      "単語: ます \t出現数: 7363 \tID: 4\n",
      "単語: か \t出現数: 7285 \tID: 5\n",
      "単語: 、 \t出現数: 6959 \tID: 6\n",
      "単語: に \t出現数: 6914 \tID: 7\n",
      "単語: が \t出現数: 6893 \tID: 8\n",
      "単語: は \t出現数: 6793 \tID: 9\n",
      "words kinds: 6961 words>=2: 6649\n"
     ]
    }
   ],
   "source": [
    "sen = All_df['sen'].values\n",
    "sen_pre = All_df['sen_pre'].values\n",
    "\n",
    "# 単語辞書の作成\n",
    "wd_set = Counter([x for s in (sen + sen_pre) for x in s])\n",
    "wd_ary = np.array(list(wd_set.keys()))\n",
    "wd_cnt = np.array(list(wd_set.values()))\n",
    "\n",
    "# 出現頻度順にソート\n",
    "wd_ary = wd_ary[np.argsort(wd_cnt)[::-1]]\n",
    "wd_cnt.sort()\n",
    "wd_cnt = wd_cnt[::-1]\n",
    "\n",
    "# 単語ID辞書の作成\n",
    "wd_to_id = {wd: i for i, wd in enumerate(wd_ary)}\n",
    "\n",
    "# Top10の単語を出力\n",
    "for i in range(10):\n",
    "    print(\"単語:\",\n",
    "          list(wd_ary)[i], \"\\t出現数:\",\n",
    "          list(wd_cnt)[i], \"\\tID:\", wd_to_id[list(wd_ary)[i]])\n",
    "\n",
    "# 出現数CUT_OFF以下の単語のIDを統一\n",
    "CUT_OFF = 2\n",
    "print(\"words kinds:\", len(wd_cnt), \"words>=\" + str(CUT_OFF) + \":\",\n",
    "      np.sum(wd_cnt >= CUT_OFF))\n",
    "other_id = np.sum(wd_cnt >= CUT_OFF)\n",
    "wd_to_id.update({wd: other_id for wd in wd_ary[wd_cnt < CUT_OFF]})\n",
    "id_to_wd = {wd_to_id[wd]: wd for wd in wd_to_id.keys()}\n",
    "\n",
    "# senとsen_preの単語をIDに変換\n",
    "sen_id = []\n",
    "sen_pre_id = []\n",
    "for s, s_pre in zip(sen, sen_pre):\n",
    "    sen_id.append([str(wd_to_id[wd]) for wd in s])\n",
    "    sen_pre_id.append([str(wd_to_id[wd]) for wd in s_pre])\n",
    "\n",
    "# 新し列としてAll_dfに追加\n",
    "All_df.insert(loc=0, column='sen_id', value=sen_id)\n",
    "All_df.insert(loc=1, column='sen_pre_id', value=sen_pre_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sen_id</th>\n",
       "      <th>sen_pre_id</th>\n",
       "      <th>sen</th>\n",
       "      <th>sen_pre</th>\n",
       "      <th>say_id</th>\n",
       "      <th>reply_id</th>\n",
       "      <th>group_id</th>\n",
       "      <th>name</th>\n",
       "      <th>body</th>\n",
       "      <th>16types_a</th>\n",
       "      <th>16types_b</th>\n",
       "      <th>argument_a</th>\n",
       "      <th>argument_b</th>\n",
       "      <th>epistemic_a</th>\n",
       "      <th>epistemic_b</th>\n",
       "      <th>social_a</th>\n",
       "      <th>social_b</th>\n",
       "      <th>coordination_a</th>\n",
       "      <th>coordination_b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[39, 35, 12, 4, 18, 0]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[よろしく, お願い, し, ます, ！, EOS]</td>\n",
       "      <td>[EOS]</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1234568</td>\n",
       "      <td>まこぴす</td>\n",
       "      <td>よろしくお願いします！</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[39, 35, 12, 4, 0]</td>\n",
       "      <td>[39, 35, 12, 4, 18, 0]</td>\n",
       "      <td>[よろしく, お願い, し, ます, EOS]</td>\n",
       "      <td>[よろしく, お願い, し, ます, ！, EOS]</td>\n",
       "      <td>31</td>\n",
       "      <td>-1</td>\n",
       "      <td>1234568</td>\n",
       "      <td>哲</td>\n",
       "      <td>よろしくお願いします</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[415, 19, 1, 8, 2253, 3, 0, 39, 35, 12, 4, 18, 0]</td>\n",
       "      <td>[39, 35, 12, 4, 0]</td>\n",
       "      <td>[名前, な, の, が, 恥ずかしい, です, EOS, よろしく, お願い, し, ます...</td>\n",
       "      <td>[よろしく, お願い, し, ます, EOS]</td>\n",
       "      <td>70</td>\n",
       "      <td>-1</td>\n",
       "      <td>1234568</td>\n",
       "      <td>仙波</td>\n",
       "      <td>名前なのが恥ずかしいです…\\nよろしくお願いします！</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sen_id              sen_pre_id  \\\n",
       "0                             [39, 35, 12, 4, 18, 0]                     [0]   \n",
       "1                                 [39, 35, 12, 4, 0]  [39, 35, 12, 4, 18, 0]   \n",
       "2  [415, 19, 1, 8, 2253, 3, 0, 39, 35, 12, 4, 18, 0]      [39, 35, 12, 4, 0]   \n",
       "\n",
       "                                                 sen  \\\n",
       "0                         [よろしく, お願い, し, ます, ！, EOS]   \n",
       "1                            [よろしく, お願い, し, ます, EOS]   \n",
       "2  [名前, な, の, が, 恥ずかしい, です, EOS, よろしく, お願い, し, ます...   \n",
       "\n",
       "                      sen_pre say_id reply_id group_id  name  \\\n",
       "0                       [EOS]      1       -1  1234568  まこぴす   \n",
       "1  [よろしく, お願い, し, ます, ！, EOS]     31       -1  1234568     哲   \n",
       "2     [よろしく, お願い, し, ます, EOS]     70       -1  1234568    仙波   \n",
       "\n",
       "                         body 16types_a 16types_b argument_a argument_b  \\\n",
       "0                 よろしくお願いします！         5         5          1          1   \n",
       "1                  よろしくお願いします         5         5          1          1   \n",
       "2  名前なのが恥ずかしいです…\\nよろしくお願いします！         5         5          1          1   \n",
       "\n",
       "  epistemic_a epistemic_b social_a social_b coordination_a coordination_b  \n",
       "0           1           1        0        0              0              0  \n",
       "1           1           1        0        0              0              0  \n",
       "2           1           1        0        0              0              0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "All_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec を利用し、単語のベクトル辞書を作成する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_sen_length: 292\n"
     ]
    }
   ],
   "source": [
    "sen_id = All_df['sen_id'].values\n",
    "sen_pre_id = All_df['sen_pre_id'].values\n",
    "sen_all = np.hstack((sen_id, sen_pre_id))\n",
    "\n",
    "max_sen_length = max([len(sen) for sen in sen_all])\n",
    "print(\"max_sen_length:\", max_sen_length)\n",
    "\n",
    "word_vectors_size = 200\n",
    "\n",
    "model = dh.get_w2v_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# （重要）各センテンスの長さを66に統一する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_sen_length: 66\n"
     ]
    }
   ],
   "source": [
    "All_df['sen_id'] = [x[:66] for x in All_df['sen_id']]\n",
    "All_df['sen_pre_id'] = [x[:66] for x in All_df['sen_pre_id']]\n",
    "\n",
    "sen_all = np.hstack((All_df['sen_id'].values, All_df['sen_pre_id'].values))\n",
    "max_sen_length = max([len(sen) for sen in sen_all])\n",
    "print(\"max_sen_length:\", max_sen_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データの準備\n",
    "* sentences dataをpaddingし、word vectorsによりfeature vectorsを作る\n",
    "* labels dataをone hotの型に変換する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "全データ(All_df)： (12012, 19)\n",
      "重複投稿を排除したデータ(All_drop_df)： (11357, 19) \n",
      "\n",
      "input data(sen)： (12012, 66, 200)\n",
      "input data(sen_pre)： (12012, 66, 200)\n"
     ]
    }
   ],
   "source": [
    "# データの整理（一致、重複）\n",
    "print(\"全データ(All_df)：\", All_df.shape)\n",
    "All_drop_df = All_df.drop_duplicates(subset=['body', 'name']).reset_index(drop=True)\n",
    "print(\"重複投稿を排除したデータ(All_drop_df)：\", All_drop_df.shape, \"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "all_sen = All_df['sen_id'].values\n",
    "all_sen = np.array([np.array(x, dtype=np.int32) for x in all_sen])\n",
    "x = dh.sen_to_fv(all_sen, max_sen_length, model, False)\n",
    "print(\"input data(sen)：\",x.shape)\n",
    "all_sen_pre = All_df['sen_pre_id'].values\n",
    "all_sen_pre = np.array([np.array(x, dtype=np.int32) for x in all_sen_pre])\n",
    "x_pre = dh.sen_to_fv(all_sen_pre, max_sen_length, model, False)\n",
    "print(\"input data(sen_pre)：\",x_pre.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16types_a： Counter({1: 2425, 2: 1938, 6: 1307, 3: 1301, 4: 1230, 5: 1224, 8: 550, 9: 510, 7: 393, 14: 310, 15: 218, 11: 184, 0: 143, 10: 123, 12: 100, 13: 56}) \n",
      "\n",
      "argument_a： Counter({1: 5804, 2: 3204, 0: 2352, 3: 509, 4: 120, 5: 23}) \n",
      "\n",
      "epistemic_a： Counter({1: 5244, 2: 4415, 0: 2054, 3: 299}) \n",
      "\n",
      "input data(t16_a)： (12012, 16)\n",
      "input data(arg_a)： (12012, 6)\n",
      "input data(epi_a)： (12012, 4)\n",
      "(1202, 16)\n",
      "(1202, 6)\n",
      "(1202, 4)\n"
     ]
    }
   ],
   "source": [
    "# 16types-------------------------------------\n",
    "print(\"16types_a：\", Counter(All_df['16types_a']), \"\\n\")\n",
    "print(\"argument_a：\", Counter(All_df['argument_a']), \"\\n\")\n",
    "print(\"epistemic_a：\", Counter(All_df['epistemic_a']), \"\\n\")\n",
    "\n",
    "label_t16_a = All_df['16types_a'].values\n",
    "label_t16_a = np.array(label_t16_a, dtype=np.int32)\n",
    "y_t16_a = dh.labels_to_one_hot(label_t16_a, 16)\n",
    "print(\"input data(t16_a)：\", y_t16_a.shape)\n",
    "\n",
    "label_arg_a = All_df['argument_a'].values\n",
    "label_arg_a = np.array(label_arg_a, dtype=np.int32)\n",
    "y_arg_a = dh.labels_to_one_hot(label_arg_a, 6)\n",
    "print(\"input data(arg_a)：\", y_arg_a.shape)\n",
    "\n",
    "\n",
    "label_epi_a = All_df['epistemic_a'].values\n",
    "label_epi_a = np.array(label_epi_a, dtype=np.int32)\n",
    "y_epi_a = dh.labels_to_one_hot(label_epi_a, 4)\n",
    "print(\"input data(epi_a)：\", y_epi_a.shape)\n",
    "\n",
    "\n",
    "data = dh.set_data_sets_2(x, y_t16_a, y_arg_a, y_epi_a)\n",
    "\n",
    "\n",
    "print(data.test.labels_1.shape)\n",
    "print(data.test.labels_2.shape)\n",
    "print(data.test.labels_3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168\n"
     ]
    }
   ],
   "source": [
    "# Network Parameters\n",
    "num_input = 200\n",
    "num_hidden = 200\n",
    "num_classes_1 = 16\n",
    "num_classes_2 = 6\n",
    "num_classes_3 = 4\n",
    "train_dropout = 1.0\n",
    "test_dropout = 1.0\n",
    "embed_dim = word_vectors_size\n",
    "sents_len = max_sen_length\n",
    "\n",
    "\n",
    "# Training Parameters\n",
    "learning_rate = 0.01\n",
    "batch_size = 64\n",
    "total_batch = int(data.train.num_examples / batch_size)\n",
    "print(total_batch)\n",
    "training_epochs = 100\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(dtype=tf.float32, shape=[None, sents_len, embed_dim])\n",
    "Y_1 = tf.placeholder(dtype=tf.float32, shape=[None, num_classes_1])\n",
    "Y_2 = tf.placeholder(dtype=tf.float32, shape=[None, num_classes_2])\n",
    "Y_3 = tf.placeholder(dtype=tf.float32, shape=[None, num_classes_3])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights_1 = {\n",
    "    'h1': tf.Variable(tf.truncated_normal(stddev=0.1, shape=[num_input, num_hidden])),\n",
    "    'out': tf.Variable(tf.truncated_normal(stddev=0.1, shape=[num_hidden, num_classes_1]))\n",
    "}\n",
    "biases_1 = {\n",
    "    'h1': tf.Variable(tf.constant(value=0.1, shape=[num_hidden])),\n",
    "    'out': tf.Variable(tf.constant(value=0.1, shape=[num_classes_1]))\n",
    "}\n",
    "\n",
    "\n",
    "weights_2 = {\n",
    "    'h1': tf.Variable(tf.truncated_normal(stddev=0.1, shape=[num_input, num_hidden])),\n",
    "    'out': tf.Variable(tf.truncated_normal(stddev=0.1, shape=[num_hidden, num_classes_2]))\n",
    "}\n",
    "biases_2 = {\n",
    "    'h1': tf.Variable(tf.constant(value=0.1, shape=[num_hidden])),\n",
    "    'out': tf.Variable(tf.constant(value=0.1, shape=[num_classes_2]))\n",
    "}\n",
    "\n",
    "weights_3 = {\n",
    "    'h1': tf.Variable(tf.truncated_normal(stddev=0.1, shape=[num_input, num_hidden])),\n",
    "    'out': tf.Variable(tf.truncated_normal(stddev=0.1, shape=[num_hidden, num_classes_3]))\n",
    "}\n",
    "biases_3 = {\n",
    "    'h1': tf.Variable(tf.constant(value=0.1, shape=[num_hidden])),\n",
    "    'out': tf.Variable(tf.constant(value=0.1, shape=[num_classes_3]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "def NN(x, weights, biases, dropout):\n",
    "\n",
    "    avg = tf.reduce_mean(x, axis=1) # [None, embed_dim]\n",
    "\n",
    "    h1 = tf.add(tf.matmul(avg, weights['h1']), biases['h1'])\n",
    "    h1_relu = tf.nn.relu(h1)\n",
    "    \n",
    "    h1_drop = tf.nn.dropout(h1_relu, dropout)\n",
    "    \n",
    "    out = tf.add(tf.matmul(h1_drop, weights['out']), biases['out'])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct model\n",
    "y_pred_1 = NN(X, weights_1, biases_1, keep_prob)\n",
    "y_pred_2 = NN(X, weights_2, biases_2, keep_prob)\n",
    "y_pred_3 = NN(X, weights_3, biases_3, keep_prob)\n",
    "\n",
    "# y_softmax = tf.nn.softmax(y_pred)\n",
    "\n",
    "# Define loss and optimizer\n",
    "# type 1(old):\n",
    "# loss = tf.reduce_mean(\n",
    "#     -tf.reduce_sum(Y * tf.log(y_softmax), reduction_indices=[1]))\n",
    "# type 2(server):\n",
    "# loss = tf.reduce_mean(\n",
    "#     tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=y_pred))\n",
    "# type 3(new):\n",
    "loss_1 = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y_1, logits=y_pred_1))\n",
    "\n",
    "loss_2 = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y_2, logits=y_pred_2))\n",
    "\n",
    "loss_3 = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y_3, logits=y_pred_3))\n",
    "\n",
    "optimizer_1 = tf.train.AdamOptimizer(learning_rate).minimize(loss_1)\n",
    "optimizer_2 = tf.train.AdamOptimizer(learning_rate).minimize(loss_2)\n",
    "optimizer_3 = tf.train.AdamOptimizer(learning_rate).minimize(loss_3)\n",
    "\n",
    "# Evaluate model\n",
    "pred_1 = tf.argmax(y_pred_1, 1)\n",
    "true_1 = tf.argmax(Y_1, 1)\n",
    "correct_prediction_1 = tf.equal(pred_1, true_1)\n",
    "accuracy_1 = tf.reduce_mean(tf.cast(correct_prediction_1, tf.float32))\n",
    "\n",
    "\n",
    "pred_2 = tf.argmax(y_pred_2, 1)\n",
    "true_2 = tf.argmax(Y_2, 1)\n",
    "correct_prediction_2 = tf.equal(pred_2, true_2)\n",
    "accuracy_2 = tf.reduce_mean(tf.cast(correct_prediction_2, tf.float32))\n",
    "\n",
    "pred_3 = tf.argmax(y_pred_3, 1)\n",
    "true_3 = tf.argmax(Y_3, 1)\n",
    "correct_prediction_3 = tf.equal(pred_3, true_3)\n",
    "accuracy_3 = tf.reduce_mean(tf.cast(correct_prediction_3, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kinten/.pyenv/versions/3.6.5/lib/python3.6/site-packages/tensorflow/python/client/session.py:1711: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep:  1 Bc: 168 | train_1=0.438 test_1=0.363 loss_1=2.126 | train_2=0.641 test_2=0.547 loss_2=1.122 | train_3=0.719 test_3=0.605 loss_3=1.040\n",
      "Ep:  2 Bc: 168 | train_1=0.344 test_1=0.379 loss_1=1.910 | train_2=0.578 test_2=0.522 loss_2=1.052 | train_3=0.656 test_3=0.582 loss_3=0.945\n",
      "Ep:  3 Bc: 168 | train_1=0.422 test_1=0.363 loss_1=1.849 | train_2=0.688 test_2=0.527 loss_2=1.036 | train_3=0.656 test_3=0.595 loss_3=0.945\n",
      "Ep:  4 Bc: 168 | train_1=0.531 test_1=0.423 loss_1=1.816 | train_2=0.688 test_2=0.543 loss_2=1.036 | train_3=0.625 test_3=0.611 loss_3=0.941\n",
      "Ep:  5 Bc: 168 | train_1=0.391 test_1=0.425 loss_1=1.780 | train_2=0.578 test_2=0.527 loss_2=1.032 | train_3=0.656 test_3=0.587 loss_3=0.933\n",
      "Ep:  6 Bc: 168 | train_1=0.438 test_1=0.437 loss_1=1.764 | train_2=0.625 test_2=0.568 loss_2=1.030 | train_3=0.609 test_3=0.615 loss_3=0.934\n",
      "Ep:  7 Bc: 168 | train_1=0.406 test_1=0.413 loss_1=1.733 | train_2=0.625 test_2=0.557 loss_2=1.018 | train_3=0.609 test_3=0.617 loss_3=0.922\n",
      "Ep:  8 Bc: 168 | train_1=0.516 test_1=0.387 loss_1=1.720 | train_2=0.641 test_2=0.549 loss_2=1.029 | train_3=0.656 test_3=0.618 loss_3=0.928\n",
      "Ep:  9 Bc: 168 | train_1=0.484 test_1=0.453 loss_1=1.719 | train_2=0.609 test_2=0.568 loss_2=1.020 | train_3=0.625 test_3=0.620 loss_3=0.920\n",
      "Ep: 10 Bc: 168 | train_1=0.453 test_1=0.445 loss_1=1.707 | train_2=0.797 test_2=0.570 loss_2=1.008 | train_3=0.828 test_3=0.616 loss_3=0.921\n",
      "Ep: 11 Bc: 168 | train_1=0.281 test_1=0.433 loss_1=1.714 | train_2=0.562 test_2=0.563 loss_2=1.017 | train_3=0.594 test_3=0.623 loss_3=0.917\n",
      "Ep: 12 Bc: 168 | train_1=0.453 test_1=0.447 loss_1=1.685 | train_2=0.641 test_2=0.548 loss_2=1.004 | train_3=0.641 test_3=0.611 loss_3=0.910\n",
      "Ep: 13 Bc: 168 | train_1=0.453 test_1=0.438 loss_1=1.701 | train_2=0.531 test_2=0.556 loss_2=1.012 | train_3=0.578 test_3=0.619 loss_3=0.919\n",
      "Ep: 14 Bc: 168 | train_1=0.484 test_1=0.441 loss_1=1.698 | train_2=0.562 test_2=0.559 loss_2=1.008 | train_3=0.641 test_3=0.616 loss_3=0.920\n",
      "Ep: 15 Bc: 168 | train_1=0.469 test_1=0.443 loss_1=1.680 | train_2=0.625 test_2=0.567 loss_2=0.996 | train_3=0.609 test_3=0.622 loss_3=0.909\n",
      "Ep: 16 Bc: 168 | train_1=0.438 test_1=0.462 loss_1=1.664 | train_2=0.688 test_2=0.562 loss_2=0.999 | train_3=0.766 test_3=0.622 loss_3=0.905\n",
      "Ep: 17 Bc: 168 | train_1=0.469 test_1=0.461 loss_1=1.662 | train_2=0.625 test_2=0.558 loss_2=0.994 | train_3=0.656 test_3=0.626 loss_3=0.902\n",
      "Ep: 18 Bc: 168 | train_1=0.344 test_1=0.446 loss_1=1.682 | train_2=0.594 test_2=0.563 loss_2=0.999 | train_3=0.641 test_3=0.616 loss_3=0.909\n",
      "Ep: 19 Bc: 168 | train_1=0.594 test_1=0.475 loss_1=1.654 | train_2=0.562 test_2=0.562 loss_2=0.994 | train_3=0.609 test_3=0.619 loss_3=0.908\n",
      "Ep: 20 Bc: 168 | train_1=0.469 test_1=0.463 loss_1=1.648 | train_2=0.703 test_2=0.561 loss_2=0.990 | train_3=0.688 test_3=0.620 loss_3=0.901\n",
      "Ep: 21 Bc: 168 | train_1=0.469 test_1=0.461 loss_1=1.643 | train_2=0.531 test_2=0.560 loss_2=0.996 | train_3=0.516 test_3=0.618 loss_3=0.903\n",
      "Ep: 22 Bc: 168 | train_1=0.469 test_1=0.466 loss_1=1.650 | train_2=0.688 test_2=0.550 loss_2=0.983 | train_3=0.703 test_3=0.613 loss_3=0.898\n",
      "Ep: 23 Bc: 168 | train_1=0.406 test_1=0.465 loss_1=1.654 | train_2=0.500 test_2=0.569 loss_2=0.995 | train_3=0.594 test_3=0.620 loss_3=0.903\n",
      "Ep: 24 Bc: 168 | train_1=0.312 test_1=0.452 loss_1=1.653 | train_2=0.609 test_2=0.559 loss_2=0.991 | train_3=0.562 test_3=0.622 loss_3=0.902\n",
      "Ep: 25 Bc: 168 | train_1=0.438 test_1=0.455 loss_1=1.628 | train_2=0.500 test_2=0.559 loss_2=0.993 | train_3=0.625 test_3=0.606 loss_3=0.900\n",
      "Ep: 26 Bc: 168 | train_1=0.453 test_1=0.457 loss_1=1.631 | train_2=0.641 test_2=0.569 loss_2=0.983 | train_3=0.656 test_3=0.621 loss_3=0.902\n",
      "Ep: 27 Bc: 168 | train_1=0.500 test_1=0.489 loss_1=1.627 | train_2=0.531 test_2=0.565 loss_2=0.996 | train_3=0.609 test_3=0.620 loss_3=0.901\n",
      "Ep: 28 Bc: 168 | train_1=0.453 test_1=0.466 loss_1=1.647 | train_2=0.562 test_2=0.555 loss_2=0.999 | train_3=0.578 test_3=0.621 loss_3=0.913\n",
      "Ep: 29 Bc: 168 | train_1=0.406 test_1=0.448 loss_1=1.627 | train_2=0.594 test_2=0.567 loss_2=0.983 | train_3=0.656 test_3=0.623 loss_3=0.898\n",
      "Ep: 30 Bc: 168 | train_1=0.500 test_1=0.467 loss_1=1.627 | train_2=0.531 test_2=0.562 loss_2=0.988 | train_3=0.688 test_3=0.621 loss_3=0.897\n",
      "Ep: 31 Bc: 168 | train_1=0.406 test_1=0.472 loss_1=1.632 | train_2=0.625 test_2=0.561 loss_2=0.984 | train_3=0.750 test_3=0.621 loss_3=0.905\n",
      "Ep: 32 Bc: 168 | train_1=0.422 test_1=0.416 loss_1=1.622 | train_2=0.641 test_2=0.559 loss_2=0.982 | train_3=0.672 test_3=0.610 loss_3=0.893\n",
      "Ep: 33 Bc: 168 | train_1=0.281 test_1=0.479 loss_1=1.629 | train_2=0.516 test_2=0.554 loss_2=0.999 | train_3=0.594 test_3=0.624 loss_3=0.904\n",
      "Ep: 34 Bc: 168 | train_1=0.500 test_1=0.468 loss_1=1.608 | train_2=0.484 test_2=0.559 loss_2=0.987 | train_3=0.672 test_3=0.625 loss_3=0.897\n",
      "Ep: 35 Bc: 168 | train_1=0.406 test_1=0.467 loss_1=1.620 | train_2=0.594 test_2=0.561 loss_2=0.982 | train_3=0.641 test_3=0.622 loss_3=0.894\n",
      "Ep: 36 Bc: 168 | train_1=0.531 test_1=0.460 loss_1=1.613 | train_2=0.578 test_2=0.553 loss_2=0.987 | train_3=0.625 test_3=0.628 loss_3=0.895\n",
      "Ep: 37 Bc: 168 | train_1=0.422 test_1=0.472 loss_1=1.593 | train_2=0.531 test_2=0.541 loss_2=0.998 | train_3=0.672 test_3=0.624 loss_3=0.895\n",
      "Ep: 38 Bc: 168 | train_1=0.375 test_1=0.467 loss_1=1.603 | train_2=0.562 test_2=0.557 loss_2=0.989 | train_3=0.609 test_3=0.619 loss_3=0.894\n",
      "Ep: 39 Bc: 168 | train_1=0.438 test_1=0.494 loss_1=1.610 | train_2=0.531 test_2=0.502 loss_2=0.989 | train_3=0.578 test_3=0.616 loss_3=0.898\n",
      "Ep: 40 Bc: 168 | train_1=0.453 test_1=0.467 loss_1=1.605 | train_2=0.609 test_2=0.548 loss_2=0.982 | train_3=0.656 test_3=0.620 loss_3=0.894\n",
      "Ep: 41 Bc: 168 | train_1=0.375 test_1=0.463 loss_1=1.606 | train_2=0.641 test_2=0.565 loss_2=0.986 | train_3=0.672 test_3=0.623 loss_3=0.894\n",
      "Ep: 42 Bc: 168 | train_1=0.438 test_1=0.412 loss_1=1.608 | train_2=0.625 test_2=0.557 loss_2=0.993 | train_3=0.781 test_3=0.622 loss_3=0.897\n",
      "Ep: 43 Bc: 168 | train_1=0.562 test_1=0.477 loss_1=1.599 | train_2=0.656 test_2=0.554 loss_2=0.977 | train_3=0.688 test_3=0.628 loss_3=0.900\n",
      "Ep: 44 Bc: 168 | train_1=0.500 test_1=0.483 loss_1=1.614 | train_2=0.547 test_2=0.551 loss_2=0.988 | train_3=0.594 test_3=0.606 loss_3=0.890\n",
      "Ep: 45 Bc: 168 | train_1=0.578 test_1=0.472 loss_1=1.596 | train_2=0.547 test_2=0.556 loss_2=0.987 | train_3=0.594 test_3=0.629 loss_3=0.891\n",
      "Ep: 46 Bc: 168 | train_1=0.500 test_1=0.479 loss_1=1.583 | train_2=0.469 test_2=0.559 loss_2=0.973 | train_3=0.578 test_3=0.622 loss_3=0.886\n",
      "Ep: 47 Bc: 168 | train_1=0.516 test_1=0.463 loss_1=1.592 | train_2=0.531 test_2=0.559 loss_2=0.987 | train_3=0.578 test_3=0.622 loss_3=0.891\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "# Training cycle\n",
    "all_test_x = data.test.vectors_1\n",
    "all_test_y_t16_a = data.test.labels_1\n",
    "all_test_y_arg_a = data.test.labels_2\n",
    "all_test_y_epi_a = data.test.labels_3\n",
    "\n",
    "start = time.time()\n",
    "for epoch_i in range(training_epochs):\n",
    "    ave_cost_1 = 0\n",
    "    ave_cost_2 = 0\n",
    "    ave_cost_3 = 0\n",
    "    for batch_i in range(total_batch):\n",
    "        batch_x, batch_y_t16_a, batch_y_arg_a, batch_y_epi_a = data.train.next_batch_2(batch_size)\n",
    "        _, c_1 = sess.run(\n",
    "            [optimizer_1, loss_1],\n",
    "            feed_dict={\n",
    "                X: batch_x,\n",
    "                Y_1: batch_y_t16_a,\n",
    "                keep_prob: train_dropout\n",
    "            })\n",
    "        _, c_2 = sess.run(\n",
    "            [optimizer_2, loss_2],\n",
    "            feed_dict={\n",
    "                X: batch_x,\n",
    "                Y_2: batch_y_arg_a,\n",
    "                keep_prob: train_dropout\n",
    "            })\n",
    "        _, c_3 = sess.run(\n",
    "            [optimizer_3, loss_3],\n",
    "            feed_dict={\n",
    "                X: batch_x,\n",
    "                Y_3: batch_y_epi_a,\n",
    "                keep_prob: train_dropout\n",
    "            })\n",
    "        ave_cost_1 += c_1 / total_batch\n",
    "        ave_cost_2 += c_2 / total_batch\n",
    "        ave_cost_3 += c_3 / total_batch\n",
    "    # Display logs per epoch step\n",
    "    if epoch_i % 1 == 0:\n",
    "        train_acc_1 = sess.run(\n",
    "            accuracy_1,\n",
    "            feed_dict={\n",
    "                X: batch_x,\n",
    "                Y_1: batch_y_t16_a,\n",
    "                keep_prob: test_dropout\n",
    "            })\n",
    "        train_acc_2 = sess.run(\n",
    "            accuracy_2,\n",
    "            feed_dict={\n",
    "                X: batch_x,\n",
    "                Y_2: batch_y_arg_a,\n",
    "                keep_prob: test_dropout\n",
    "            })\n",
    "        train_acc_3 = sess.run(\n",
    "            accuracy_3,\n",
    "            feed_dict={\n",
    "                X: batch_x,\n",
    "                Y_3: batch_y_epi_a,\n",
    "                keep_prob: test_dropout\n",
    "            })\n",
    "        \n",
    "        test_acc_1 = sess.run(\n",
    "            accuracy_1,\n",
    "            feed_dict={\n",
    "                X: all_test_x,\n",
    "                Y_1: all_test_y_t16_a,\n",
    "                keep_prob: test_dropout\n",
    "            })\n",
    "        test_acc_2 = sess.run(\n",
    "            accuracy_2,\n",
    "            feed_dict={\n",
    "                X: all_test_x,\n",
    "                Y_2: all_test_y_arg_a,\n",
    "                keep_prob: test_dropout\n",
    "            })\n",
    "        test_acc_3 = sess.run(\n",
    "            accuracy_3,\n",
    "            feed_dict={\n",
    "                X: all_test_x,\n",
    "                Y_3: all_test_y_epi_a,\n",
    "                keep_prob: test_dropout\n",
    "            })\n",
    "        print(\"Ep:%3d Bc:%4d\" % (epoch_i + 1, batch_i + 1),\n",
    "              \"| train_1=%.3f\" % train_acc_1, \"test_1=%.3f\" % test_acc_1, \"loss_1=%5.3f\" % ave_cost_1,\n",
    "              \"| train_2=%.3f\" % train_acc_2, \"test_2=%.3f\" % test_acc_2, \"loss_2=%5.3f\" % ave_cost_2,\n",
    "              \"| train_3=%.3f\" % train_acc_3, \"test_3=%.3f\" % test_acc_3, \"loss_3=%5.3f\" % ave_cost_3,)\n",
    "end = time.time()\n",
    "print(\"Process Time :%.2f s\" % (end - start))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
