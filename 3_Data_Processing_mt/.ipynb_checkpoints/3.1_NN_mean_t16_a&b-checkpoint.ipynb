{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import MeCab\n",
    "import pickle\n",
    "import numpy as np\n",
    "import data_helpers as dh\n",
    "import pandas as pd\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import MDS,TSNE\n",
    "from collections import Counter\n",
    "from collections import OrderedDict\n",
    "from gensim.models import word2vec\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sen</th>\n",
       "      <th>sen_pre</th>\n",
       "      <th>say_id</th>\n",
       "      <th>reply_id</th>\n",
       "      <th>group_id</th>\n",
       "      <th>name</th>\n",
       "      <th>body</th>\n",
       "      <th>16types_a</th>\n",
       "      <th>16types_b</th>\n",
       "      <th>argument_a</th>\n",
       "      <th>argument_b</th>\n",
       "      <th>epistemic_a</th>\n",
       "      <th>epistemic_b</th>\n",
       "      <th>social_a</th>\n",
       "      <th>social_b</th>\n",
       "      <th>coordination_a</th>\n",
       "      <th>coordination_b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[よろしく, お願い, し, ます, ！, EOS]</td>\n",
       "      <td>[EOS]</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1234568</td>\n",
       "      <td>まこぴす</td>\n",
       "      <td>よろしくお願いします！</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[よろしく, お願い, し, ます, EOS]</td>\n",
       "      <td>[よろしく, お願い, し, ます, ！, EOS]</td>\n",
       "      <td>31</td>\n",
       "      <td>-1</td>\n",
       "      <td>1234568</td>\n",
       "      <td>哲</td>\n",
       "      <td>よろしくお願いします</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[名前, な, の, が, 恥ずかしい, です, EOS, よろしく, お願い, し, ます...</td>\n",
       "      <td>[よろしく, お願い, し, ます, EOS]</td>\n",
       "      <td>70</td>\n",
       "      <td>-1</td>\n",
       "      <td>1234568</td>\n",
       "      <td>仙波</td>\n",
       "      <td>名前なのが恥ずかしいです…\\nよろしくお願いします！</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 sen  \\\n",
       "0                         [よろしく, お願い, し, ます, ！, EOS]   \n",
       "1                            [よろしく, お願い, し, ます, EOS]   \n",
       "2  [名前, な, の, が, 恥ずかしい, です, EOS, よろしく, お願い, し, ます...   \n",
       "\n",
       "                      sen_pre say_id reply_id group_id  name  \\\n",
       "0                       [EOS]      1       -1  1234568  まこぴす   \n",
       "1  [よろしく, お願い, し, ます, ！, EOS]     31       -1  1234568     哲   \n",
       "2     [よろしく, お願い, し, ます, EOS]     70       -1  1234568    仙波   \n",
       "\n",
       "                         body 16types_a 16types_b argument_a argument_b  \\\n",
       "0                 よろしくお願いします！         5         5          1          1   \n",
       "1                  よろしくお願いします         5         5          1          1   \n",
       "2  名前なのが恥ずかしいです…\\nよろしくお願いします！         5         5          1          1   \n",
       "\n",
       "  epistemic_a epistemic_b social_a social_b coordination_a coordination_b  \n",
       "0           1           1        0        0              0              0  \n",
       "1           1           1        0        0              0              0  \n",
       "2           1           1        0        0              0              0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "All_df = pd.read_pickle(\"../data/all_mecab.pickle\")\n",
    "All_df.head(3)\n",
    "\n",
    "# senとsen_preの単語をIDに変換し、新たな列としてAll_dfに追加する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# senとsen_preの単語をIDに変換し、新たな列としてAll_dfに追加する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "単語: EOS \t出現数: 29580 \tID: 0\n",
      "単語: の \t出現数: 10567 \tID: 1\n",
      "単語: て \t出現数: 7408 \tID: 2\n",
      "単語: です \t出現数: 7390 \tID: 3\n",
      "単語: ます \t出現数: 7363 \tID: 4\n",
      "単語: か \t出現数: 7285 \tID: 5\n",
      "単語: 、 \t出現数: 6959 \tID: 6\n",
      "単語: に \t出現数: 6914 \tID: 7\n",
      "単語: が \t出現数: 6893 \tID: 8\n",
      "単語: は \t出現数: 6793 \tID: 9\n",
      "words kinds: 6961 words>=2: 6649\n"
     ]
    }
   ],
   "source": [
    "sen = All_df['sen'].values\n",
    "sen_pre = All_df['sen_pre'].values\n",
    "\n",
    "# 単語辞書の作成\n",
    "wd_set = Counter([x for s in (sen + sen_pre) for x in s])\n",
    "wd_ary = np.array(list(wd_set.keys()))\n",
    "wd_cnt = np.array(list(wd_set.values()))\n",
    "\n",
    "# 出現頻度順にソート\n",
    "wd_ary = wd_ary[np.argsort(wd_cnt)[::-1]]\n",
    "wd_cnt.sort()\n",
    "wd_cnt = wd_cnt[::-1]\n",
    "\n",
    "# 単語ID辞書の作成\n",
    "wd_to_id = {wd: i for i, wd in enumerate(wd_ary)}\n",
    "\n",
    "# Top10の単語を出力\n",
    "for i in range(10):\n",
    "    print(\"単語:\",\n",
    "          list(wd_ary)[i], \"\\t出現数:\",\n",
    "          list(wd_cnt)[i], \"\\tID:\", wd_to_id[list(wd_ary)[i]])\n",
    "\n",
    "# 出現数CUT_OFF以下の単語のIDを統一\n",
    "CUT_OFF = 2\n",
    "print(\"words kinds:\", len(wd_cnt), \"words>=\" + str(CUT_OFF) + \":\",\n",
    "      np.sum(wd_cnt >= CUT_OFF))\n",
    "other_id = np.sum(wd_cnt >= CUT_OFF)\n",
    "wd_to_id.update({wd: other_id for wd in wd_ary[wd_cnt < CUT_OFF]})\n",
    "id_to_wd = {wd_to_id[wd]: wd for wd in wd_to_id.keys()}\n",
    "\n",
    "# senとsen_preの単語をIDに変換\n",
    "sen_id = []\n",
    "sen_pre_id = []\n",
    "for s, s_pre in zip(sen, sen_pre):\n",
    "    sen_id.append([str(wd_to_id[wd]) for wd in s])\n",
    "    sen_pre_id.append([str(wd_to_id[wd]) for wd in s_pre])\n",
    "\n",
    "# 新し列としてAll_dfに追加\n",
    "All_df.insert(loc=0, column='sen_id', value=sen_id)\n",
    "All_df.insert(loc=1, column='sen_pre_id', value=sen_pre_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sen_id</th>\n",
       "      <th>sen_pre_id</th>\n",
       "      <th>sen</th>\n",
       "      <th>sen_pre</th>\n",
       "      <th>say_id</th>\n",
       "      <th>reply_id</th>\n",
       "      <th>group_id</th>\n",
       "      <th>name</th>\n",
       "      <th>body</th>\n",
       "      <th>16types_a</th>\n",
       "      <th>16types_b</th>\n",
       "      <th>argument_a</th>\n",
       "      <th>argument_b</th>\n",
       "      <th>epistemic_a</th>\n",
       "      <th>epistemic_b</th>\n",
       "      <th>social_a</th>\n",
       "      <th>social_b</th>\n",
       "      <th>coordination_a</th>\n",
       "      <th>coordination_b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[39, 35, 12, 4, 18, 0]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[よろしく, お願い, し, ます, ！, EOS]</td>\n",
       "      <td>[EOS]</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1234568</td>\n",
       "      <td>まこぴす</td>\n",
       "      <td>よろしくお願いします！</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[39, 35, 12, 4, 0]</td>\n",
       "      <td>[39, 35, 12, 4, 18, 0]</td>\n",
       "      <td>[よろしく, お願い, し, ます, EOS]</td>\n",
       "      <td>[よろしく, お願い, し, ます, ！, EOS]</td>\n",
       "      <td>31</td>\n",
       "      <td>-1</td>\n",
       "      <td>1234568</td>\n",
       "      <td>哲</td>\n",
       "      <td>よろしくお願いします</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[415, 19, 1, 8, 2253, 3, 0, 39, 35, 12, 4, 18, 0]</td>\n",
       "      <td>[39, 35, 12, 4, 0]</td>\n",
       "      <td>[名前, な, の, が, 恥ずかしい, です, EOS, よろしく, お願い, し, ます...</td>\n",
       "      <td>[よろしく, お願い, し, ます, EOS]</td>\n",
       "      <td>70</td>\n",
       "      <td>-1</td>\n",
       "      <td>1234568</td>\n",
       "      <td>仙波</td>\n",
       "      <td>名前なのが恥ずかしいです…\\nよろしくお願いします！</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sen_id              sen_pre_id  \\\n",
       "0                             [39, 35, 12, 4, 18, 0]                     [0]   \n",
       "1                                 [39, 35, 12, 4, 0]  [39, 35, 12, 4, 18, 0]   \n",
       "2  [415, 19, 1, 8, 2253, 3, 0, 39, 35, 12, 4, 18, 0]      [39, 35, 12, 4, 0]   \n",
       "\n",
       "                                                 sen  \\\n",
       "0                         [よろしく, お願い, し, ます, ！, EOS]   \n",
       "1                            [よろしく, お願い, し, ます, EOS]   \n",
       "2  [名前, な, の, が, 恥ずかしい, です, EOS, よろしく, お願い, し, ます...   \n",
       "\n",
       "                      sen_pre say_id reply_id group_id  name  \\\n",
       "0                       [EOS]      1       -1  1234568  まこぴす   \n",
       "1  [よろしく, お願い, し, ます, ！, EOS]     31       -1  1234568     哲   \n",
       "2     [よろしく, お願い, し, ます, EOS]     70       -1  1234568    仙波   \n",
       "\n",
       "                         body 16types_a 16types_b argument_a argument_b  \\\n",
       "0                 よろしくお願いします！         5         5          1          1   \n",
       "1                  よろしくお願いします         5         5          1          1   \n",
       "2  名前なのが恥ずかしいです…\\nよろしくお願いします！         5         5          1          1   \n",
       "\n",
       "  epistemic_a epistemic_b social_a social_b coordination_a coordination_b  \n",
       "0           1           1        0        0              0              0  \n",
       "1           1           1        0        0              0              0  \n",
       "2           1           1        0        0              0              0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "All_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec を利用し、単語のベクトル辞書を作成する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_sen_length: 292\n"
     ]
    }
   ],
   "source": [
    "sen_id = All_df['sen_id'].values\n",
    "sen_pre_id = All_df['sen_pre_id'].values\n",
    "sen_all = np.hstack((sen_id, sen_pre_id))\n",
    "\n",
    "max_sen_length = max([len(sen) for sen in sen_all])\n",
    "print(\"max_sen_length:\", max_sen_length)\n",
    "\n",
    "word_vectors_size = 200\n",
    "\n",
    "model = dh.get_w2v_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# （重要）各センテンスの長さを66に統一する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_sen_length: 66\n"
     ]
    }
   ],
   "source": [
    "All_df['sen_id'] = [x[:66] for x in All_df['sen_id']]\n",
    "All_df['sen_pre_id'] = [x[:66] for x in All_df['sen_pre_id']]\n",
    "\n",
    "sen_all = np.hstack((All_df['sen_id'].values, All_df['sen_pre_id'].values))\n",
    "max_sen_length = max([len(sen) for sen in sen_all])\n",
    "print(\"max_sen_length:\", max_sen_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データの準備\n",
    "* sentences dataをpaddingし、word vectorsによりfeature vectorsを作る\n",
    "* labels dataをone hotの型に変換する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "全データ(All_df)： (12012, 19)\n",
      "重複投稿を排除したデータ(All_drop_df)： (11357, 19) \n",
      "\n",
      "input data(sen)： (12012, 66, 200)\n",
      "input data(sen_pre)： (12012, 66, 200)\n"
     ]
    }
   ],
   "source": [
    "# データの整理（一致、重複）\n",
    "print(\"全データ(All_df)：\", All_df.shape)\n",
    "All_drop_df = All_df.drop_duplicates(subset=['body', 'name']).reset_index(drop=True)\n",
    "print(\"重複投稿を排除したデータ(All_drop_df)：\", All_drop_df.shape, \"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "all_sen = All_df['sen_id'].values\n",
    "all_sen = np.array([np.array(x, dtype=np.int32) for x in all_sen])\n",
    "x = dh.sen_to_fv(all_sen, max_sen_length, model, False)\n",
    "print(\"input data(sen)：\",x.shape)\n",
    "all_sen_pre = All_df['sen_pre_id'].values\n",
    "all_sen_pre = np.array([np.array(x, dtype=np.int32) for x in all_sen_pre])\n",
    "x_pre = dh.sen_to_fv(all_sen_pre, max_sen_length, model, False)\n",
    "print(\"input data(sen_pre)：\",x_pre.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16types_a： Counter({1: 2425, 2: 1938, 6: 1307, 3: 1301, 4: 1230, 5: 1224, 8: 550, 9: 510, 7: 393, 14: 310, 15: 218, 11: 184, 0: 143, 10: 123, 12: 100, 13: 56}) \n",
      "\n",
      "16types_b： Counter({1: 2759, 2: 1994, 4: 1291, 5: 1262, 3: 1243, 6: 1131, 7: 729, 8: 407, 14: 327, 15: 231, 11: 193, 9: 183, 12: 113, 13: 90, 10: 34, 0: 25}) \n",
      "\n",
      "input data(t16_a)： (12012, 16)\n",
      "input data(t16_b)： (12012, 16)\n"
     ]
    }
   ],
   "source": [
    "# 16types-------------------------------------\n",
    "print(\"16types_a：\", Counter(All_df['16types_a']), \"\\n\")\n",
    "print(\"16types_b：\", Counter(All_df['16types_b']), \"\\n\")\n",
    "\n",
    "label_t16_a = All_df['16types_a'].values\n",
    "label_t16_a = np.array(label_t16_a, dtype=np.int32)\n",
    "y_t16_a = dh.labels_to_one_hot(label_t16_a, 16)\n",
    "print(\"input data(t16_a)：\", y_t16_a.shape)\n",
    "\n",
    "label_t16_b = All_df['16types_b'].values\n",
    "label_t_b = np.array(label_t16_b, dtype=np.int32)\n",
    "y_t16_b = dh.labels_to_one_hot(label_t_b, 16)\n",
    "print(\"input data(t16_b)：\", y_t16_b.shape)\n",
    "\n",
    "t16_data = dh.set_data_sets(x, y_t16_a, y_t16_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168\n"
     ]
    }
   ],
   "source": [
    "# Network Parameters\n",
    "num_input = 200\n",
    "num_hidden = 200\n",
    "num_classes = 16\n",
    "train_dropout = 1.0\n",
    "test_dropout = 1.0\n",
    "embed_dim = word_vectors_size\n",
    "sents_len = max_sen_length\n",
    "\n",
    "\n",
    "# Training Parameters\n",
    "learning_rate = 0.01\n",
    "batch_size = 64\n",
    "total_batch = int(t16_data.train.num_examples / batch_size)\n",
    "print(total_batch)\n",
    "training_epochs = 100\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(dtype=tf.float32, shape=[None, sents_len, embed_dim])\n",
    "Y_a = tf.placeholder(dtype=tf.float32, shape=[None, num_classes])\n",
    "Y_b = tf.placeholder(dtype=tf.float32, shape=[None, num_classes])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights_a = {\n",
    "    'h1': tf.Variable(tf.truncated_normal(stddev=0.1, shape=[num_input, num_hidden])),\n",
    "    'out': tf.Variable(tf.truncated_normal(stddev=0.1, shape=[num_hidden, num_classes]))\n",
    "}\n",
    "biases_a = {\n",
    "    'h1': tf.Variable(tf.constant(value=0.1, shape=[num_hidden])),\n",
    "    'out': tf.Variable(tf.constant(value=0.1, shape=[num_classes]))\n",
    "}\n",
    "\n",
    "\n",
    "weights_b = {\n",
    "    'h1': tf.Variable(tf.truncated_normal(stddev=0.1, shape=[num_input, num_hidden])),\n",
    "    'out': tf.Variable(tf.truncated_normal(stddev=0.1, shape=[num_hidden, num_classes]))\n",
    "}\n",
    "biases_b = {\n",
    "    'h1': tf.Variable(tf.constant(value=0.1, shape=[num_hidden])),\n",
    "    'out': tf.Variable(tf.constant(value=0.1, shape=[num_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "def NN(x, weights, biases, dropout):\n",
    "\n",
    "    avg = tf.reduce_mean(x, axis=1) # [None, embed_dim]\n",
    "\n",
    "    h1 = tf.add(tf.matmul(avg, weights['h1']), biases['h1'])\n",
    "    h1_relu = tf.nn.relu(h1)\n",
    "    \n",
    "    h1_drop = tf.nn.dropout(h1_relu, dropout)\n",
    "    \n",
    "    out = tf.add(tf.matmul(h1_drop, weights['out']), biases['out'])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct model\n",
    "y_pred_a = NN(X, weights_a, biases_a, keep_prob)\n",
    "y_pred_b = NN(X, weights_b, biases_b, keep_prob)\n",
    "\n",
    "# y_softmax = tf.nn.softmax(y_pred)\n",
    "\n",
    "# Define loss and optimizer\n",
    "# type 1(old):\n",
    "# loss = tf.reduce_mean(\n",
    "#     -tf.reduce_sum(Y * tf.log(y_softmax), reduction_indices=[1]))\n",
    "# type 2(server):\n",
    "# loss = tf.reduce_mean(\n",
    "#     tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=y_pred))\n",
    "# type 3(new):\n",
    "loss_a = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y_a, logits=y_pred_a))\n",
    "\n",
    "loss_b = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y_b, logits=y_pred_b))\n",
    "\n",
    "\n",
    "optimizer_a = tf.train.AdamOptimizer(learning_rate).minimize(loss_a)\n",
    "optimizer_b = tf.train.AdamOptimizer(learning_rate).minimize(loss_b)\n",
    "\n",
    "\n",
    "# Evaluate model\n",
    "pred_a = tf.argmax(y_pred_a, 1)\n",
    "true_a = tf.argmax(Y_a, 1)\n",
    "correct_prediction_a = tf.equal(pred_a, true_a)\n",
    "accuracy_a = tf.reduce_mean(tf.cast(correct_prediction_a, tf.float32))\n",
    "\n",
    "\n",
    "pred_b = tf.argmax(y_pred_b, 1)\n",
    "true_b = tf.argmax(Y_b, 1)\n",
    "correct_prediction_b = tf.equal(pred_b, true_b)\n",
    "accuracy_b = tf.reduce_mean(tf.cast(correct_prediction_b, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep:  1 Bc: 168 | train_a=0.312 test_a=0.238 loss_a=2.194 | train_b=0.266 test_b=0.255 loss_b=2.208\n",
      "Ep:  2 Bc: 168 | train_a=0.406 test_a=0.370 loss_a=1.985 | train_b=0.438 test_b=0.371 loss_b=1.988\n",
      "Ep:  3 Bc: 168 | train_a=0.359 test_a=0.359 loss_a=1.893 | train_b=0.344 test_b=0.364 loss_b=1.892\n",
      "Ep:  4 Bc: 168 | train_a=0.438 test_a=0.402 loss_a=1.839 | train_b=0.422 test_b=0.416 loss_b=1.834\n",
      "Ep:  5 Bc: 168 | train_a=0.406 test_a=0.404 loss_a=1.830 | train_b=0.359 test_b=0.423 loss_b=1.819\n",
      "Ep:  6 Bc: 168 | train_a=0.281 test_a=0.396 loss_a=1.790 | train_b=0.281 test_b=0.365 loss_b=1.791\n",
      "Ep:  7 Bc: 168 | train_a=0.297 test_a=0.385 loss_a=1.774 | train_b=0.281 test_b=0.384 loss_b=1.780\n",
      "Ep:  8 Bc: 168 | train_a=0.469 test_a=0.435 loss_a=1.752 | train_b=0.438 test_b=0.433 loss_b=1.750\n",
      "Ep:  9 Bc: 168 | train_a=0.438 test_a=0.449 loss_a=1.742 | train_b=0.422 test_b=0.453 loss_b=1.750\n",
      "Ep: 10 Bc: 168 | train_a=0.453 test_a=0.453 loss_a=1.726 | train_b=0.438 test_b=0.465 loss_b=1.730\n",
      "Ep: 11 Bc: 168 | train_a=0.422 test_a=0.463 loss_a=1.707 | train_b=0.422 test_b=0.465 loss_b=1.710\n",
      "Ep: 12 Bc: 168 | train_a=0.484 test_a=0.429 loss_a=1.692 | train_b=0.516 test_b=0.433 loss_b=1.704\n",
      "Ep: 13 Bc: 168 | train_a=0.562 test_a=0.455 loss_a=1.709 | train_b=0.578 test_b=0.457 loss_b=1.702\n",
      "Ep: 14 Bc: 168 | train_a=0.422 test_a=0.434 loss_a=1.686 | train_b=0.422 test_b=0.440 loss_b=1.687\n",
      "Ep: 15 Bc: 168 | train_a=0.500 test_a=0.443 loss_a=1.686 | train_b=0.453 test_b=0.470 loss_b=1.686\n",
      "Ep: 16 Bc: 168 | train_a=0.516 test_a=0.443 loss_a=1.681 | train_b=0.516 test_b=0.446 loss_b=1.675\n",
      "Ep: 17 Bc: 168 | train_a=0.531 test_a=0.456 loss_a=1.676 | train_b=0.516 test_b=0.460 loss_b=1.676\n",
      "Ep: 18 Bc: 168 | train_a=0.469 test_a=0.423 loss_a=1.667 | train_b=0.484 test_b=0.436 loss_b=1.662\n",
      "Ep: 19 Bc: 168 | train_a=0.422 test_a=0.428 loss_a=1.671 | train_b=0.406 test_b=0.460 loss_b=1.678\n",
      "Ep: 20 Bc: 168 | train_a=0.438 test_a=0.457 loss_a=1.675 | train_b=0.422 test_b=0.455 loss_b=1.676\n",
      "Ep: 21 Bc: 168 | train_a=0.422 test_a=0.458 loss_a=1.660 | train_b=0.359 test_b=0.455 loss_b=1.659\n",
      "Ep: 22 Bc: 168 | train_a=0.391 test_a=0.445 loss_a=1.657 | train_b=0.359 test_b=0.449 loss_b=1.657\n",
      "Ep: 23 Bc: 168 | train_a=0.469 test_a=0.464 loss_a=1.653 | train_b=0.469 test_b=0.463 loss_b=1.650\n",
      "Ep: 24 Bc: 168 | train_a=0.438 test_a=0.433 loss_a=1.641 | train_b=0.469 test_b=0.445 loss_b=1.652\n",
      "Ep: 25 Bc: 168 | train_a=0.422 test_a=0.452 loss_a=1.632 | train_b=0.453 test_b=0.446 loss_b=1.637\n",
      "Ep: 26 Bc: 168 | train_a=0.578 test_a=0.479 loss_a=1.631 | train_b=0.531 test_b=0.467 loss_b=1.626\n",
      "Ep: 27 Bc: 168 | train_a=0.391 test_a=0.454 loss_a=1.643 | train_b=0.344 test_b=0.465 loss_b=1.654\n",
      "Ep: 28 Bc: 168 | train_a=0.469 test_a=0.449 loss_a=1.632 | train_b=0.516 test_b=0.471 loss_b=1.634\n",
      "Ep: 29 Bc: 168 | train_a=0.422 test_a=0.466 loss_a=1.631 | train_b=0.453 test_b=0.480 loss_b=1.633\n",
      "Ep: 30 Bc: 168 | train_a=0.500 test_a=0.478 loss_a=1.642 | train_b=0.469 test_b=0.469 loss_b=1.644\n",
      "Ep: 31 Bc: 168 | train_a=0.531 test_a=0.468 loss_a=1.629 | train_b=0.547 test_b=0.474 loss_b=1.626\n",
      "Ep: 32 Bc: 168 | train_a=0.406 test_a=0.472 loss_a=1.626 | train_b=0.406 test_b=0.486 loss_b=1.624\n",
      "Ep: 33 Bc: 168 | train_a=0.500 test_a=0.475 loss_a=1.612 | train_b=0.484 test_b=0.479 loss_b=1.614\n",
      "Ep: 34 Bc: 168 | train_a=0.453 test_a=0.478 loss_a=1.621 | train_b=0.516 test_b=0.466 loss_b=1.630\n",
      "Ep: 35 Bc: 168 | train_a=0.438 test_a=0.432 loss_a=1.622 | train_b=0.484 test_b=0.464 loss_b=1.621\n",
      "Ep: 36 Bc: 168 | train_a=0.469 test_a=0.468 loss_a=1.618 | train_b=0.484 test_b=0.469 loss_b=1.630\n",
      "Ep: 37 Bc: 168 | train_a=0.438 test_a=0.467 loss_a=1.620 | train_b=0.438 test_b=0.464 loss_b=1.620\n",
      "Ep: 38 Bc: 168 | train_a=0.484 test_a=0.474 loss_a=1.627 | train_b=0.484 test_b=0.482 loss_b=1.636\n",
      "Ep: 39 Bc: 168 | train_a=0.375 test_a=0.483 loss_a=1.612 | train_b=0.375 test_b=0.474 loss_b=1.617\n",
      "Ep: 40 Bc: 168 | train_a=0.422 test_a=0.478 loss_a=1.612 | train_b=0.406 test_b=0.478 loss_b=1.620\n",
      "Ep: 41 Bc: 168 | train_a=0.594 test_a=0.476 loss_a=1.625 | train_b=0.547 test_b=0.469 loss_b=1.627\n",
      "Ep: 42 Bc: 168 | train_a=0.594 test_a=0.477 loss_a=1.606 | train_b=0.578 test_b=0.471 loss_b=1.615\n",
      "Ep: 43 Bc: 168 | train_a=0.438 test_a=0.469 loss_a=1.603 | train_b=0.438 test_b=0.477 loss_b=1.607\n",
      "Ep: 44 Bc: 168 | train_a=0.516 test_a=0.470 loss_a=1.619 | train_b=0.484 test_b=0.474 loss_b=1.621\n",
      "Ep: 45 Bc: 168 | train_a=0.375 test_a=0.491 loss_a=1.605 | train_b=0.328 test_b=0.487 loss_b=1.618\n",
      "Ep: 46 Bc: 168 | train_a=0.438 test_a=0.472 loss_a=1.606 | train_b=0.422 test_b=0.441 loss_b=1.613\n",
      "Ep: 47 Bc: 168 | train_a=0.594 test_a=0.434 loss_a=1.609 | train_b=0.562 test_b=0.460 loss_b=1.615\n",
      "Ep: 48 Bc: 168 | train_a=0.469 test_a=0.477 loss_a=1.594 | train_b=0.484 test_b=0.469 loss_b=1.608\n",
      "Ep: 49 Bc: 168 | train_a=0.484 test_a=0.478 loss_a=1.598 | train_b=0.484 test_b=0.473 loss_b=1.612\n",
      "Ep: 50 Bc: 168 | train_a=0.422 test_a=0.467 loss_a=1.591 | train_b=0.422 test_b=0.481 loss_b=1.609\n",
      "Ep: 51 Bc: 168 | train_a=0.484 test_a=0.478 loss_a=1.593 | train_b=0.500 test_b=0.481 loss_b=1.601\n",
      "Ep: 52 Bc: 168 | train_a=0.547 test_a=0.481 loss_a=1.604 | train_b=0.562 test_b=0.463 loss_b=1.604\n",
      "Ep: 53 Bc: 168 | train_a=0.438 test_a=0.463 loss_a=1.592 | train_b=0.438 test_b=0.470 loss_b=1.599\n",
      "Ep: 54 Bc: 168 | train_a=0.578 test_a=0.463 loss_a=1.602 | train_b=0.516 test_b=0.478 loss_b=1.611\n",
      "Ep: 55 Bc: 168 | train_a=0.484 test_a=0.457 loss_a=1.598 | train_b=0.453 test_b=0.468 loss_b=1.609\n",
      "Ep: 56 Bc: 168 | train_a=0.422 test_a=0.462 loss_a=1.588 | train_b=0.438 test_b=0.492 loss_b=1.599\n",
      "Ep: 57 Bc: 168 | train_a=0.438 test_a=0.483 loss_a=1.591 | train_b=0.438 test_b=0.484 loss_b=1.591\n",
      "Ep: 58 Bc: 168 | train_a=0.500 test_a=0.479 loss_a=1.588 | train_b=0.438 test_b=0.481 loss_b=1.591\n",
      "Ep: 59 Bc: 168 | train_a=0.516 test_a=0.473 loss_a=1.581 | train_b=0.484 test_b=0.479 loss_b=1.594\n",
      "Ep: 60 Bc: 168 | train_a=0.500 test_a=0.467 loss_a=1.590 | train_b=0.484 test_b=0.477 loss_b=1.603\n",
      "Ep: 61 Bc: 168 | train_a=0.516 test_a=0.472 loss_a=1.587 | train_b=0.484 test_b=0.493 loss_b=1.588\n",
      "Ep: 62 Bc: 168 | train_a=0.391 test_a=0.446 loss_a=1.584 | train_b=0.344 test_b=0.467 loss_b=1.590\n",
      "Ep: 63 Bc: 168 | train_a=0.547 test_a=0.481 loss_a=1.596 | train_b=0.578 test_b=0.485 loss_b=1.600\n",
      "Ep: 64 Bc: 168 | train_a=0.500 test_a=0.491 loss_a=1.585 | train_b=0.469 test_b=0.484 loss_b=1.592\n",
      "Ep: 65 Bc: 168 | train_a=0.453 test_a=0.493 loss_a=1.589 | train_b=0.453 test_b=0.488 loss_b=1.597\n",
      "Ep: 66 Bc: 168 | train_a=0.469 test_a=0.447 loss_a=1.580 | train_b=0.469 test_b=0.476 loss_b=1.590\n",
      "Ep: 67 Bc: 168 | train_a=0.438 test_a=0.473 loss_a=1.597 | train_b=0.438 test_b=0.471 loss_b=1.607\n",
      "Ep: 68 Bc: 168 | train_a=0.531 test_a=0.483 loss_a=1.581 | train_b=0.516 test_b=0.480 loss_b=1.591\n",
      "Ep: 69 Bc: 168 | train_a=0.406 test_a=0.493 loss_a=1.589 | train_b=0.453 test_b=0.487 loss_b=1.590\n",
      "Ep: 70 Bc: 168 | train_a=0.406 test_a=0.466 loss_a=1.579 | train_b=0.438 test_b=0.488 loss_b=1.583\n",
      "Ep: 71 Bc: 168 | train_a=0.422 test_a=0.473 loss_a=1.588 | train_b=0.375 test_b=0.486 loss_b=1.588\n",
      "Ep: 72 Bc: 168 | train_a=0.547 test_a=0.484 loss_a=1.569 | train_b=0.562 test_b=0.493 loss_b=1.581\n",
      "Ep: 73 Bc: 168 | train_a=0.484 test_a=0.468 loss_a=1.567 | train_b=0.516 test_b=0.480 loss_b=1.584\n",
      "Ep: 74 Bc: 168 | train_a=0.344 test_a=0.437 loss_a=1.578 | train_b=0.406 test_b=0.452 loss_b=1.579\n",
      "Ep: 75 Bc: 168 | train_a=0.484 test_a=0.465 loss_a=1.579 | train_b=0.516 test_b=0.490 loss_b=1.584\n",
      "Ep: 76 Bc: 168 | train_a=0.406 test_a=0.474 loss_a=1.578 | train_b=0.359 test_b=0.473 loss_b=1.576\n",
      "Ep: 77 Bc: 168 | train_a=0.531 test_a=0.464 loss_a=1.585 | train_b=0.578 test_b=0.484 loss_b=1.600\n",
      "Ep: 78 Bc: 168 | train_a=0.422 test_a=0.478 loss_a=1.578 | train_b=0.453 test_b=0.442 loss_b=1.584\n",
      "Ep: 79 Bc: 168 | train_a=0.453 test_a=0.461 loss_a=1.568 | train_b=0.469 test_b=0.477 loss_b=1.580\n",
      "Ep: 80 Bc: 168 | train_a=0.453 test_a=0.488 loss_a=1.577 | train_b=0.422 test_b=0.495 loss_b=1.573\n",
      "Ep: 81 Bc: 168 | train_a=0.406 test_a=0.483 loss_a=1.569 | train_b=0.391 test_b=0.478 loss_b=1.583\n",
      "Ep: 82 Bc: 168 | train_a=0.531 test_a=0.500 loss_a=1.566 | train_b=0.516 test_b=0.490 loss_b=1.580\n",
      "Ep: 83 Bc: 168 | train_a=0.438 test_a=0.459 loss_a=1.570 | train_b=0.422 test_b=0.471 loss_b=1.584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 84 Bc: 168 | train_a=0.578 test_a=0.487 loss_a=1.568 | train_b=0.547 test_b=0.492 loss_b=1.572\n",
      "Ep: 85 Bc: 168 | train_a=0.578 test_a=0.482 loss_a=1.575 | train_b=0.547 test_b=0.490 loss_b=1.587\n",
      "Ep: 86 Bc: 168 | train_a=0.453 test_a=0.498 loss_a=1.572 | train_b=0.469 test_b=0.479 loss_b=1.574\n",
      "Ep: 87 Bc: 168 | train_a=0.516 test_a=0.448 loss_a=1.578 | train_b=0.469 test_b=0.453 loss_b=1.585\n",
      "Ep: 88 Bc: 168 | train_a=0.484 test_a=0.458 loss_a=1.565 | train_b=0.500 test_b=0.465 loss_b=1.568\n",
      "Ep: 89 Bc: 168 | train_a=0.391 test_a=0.490 loss_a=1.562 | train_b=0.391 test_b=0.494 loss_b=1.573\n",
      "Ep: 90 Bc: 168 | train_a=0.469 test_a=0.476 loss_a=1.569 | train_b=0.484 test_b=0.501 loss_b=1.580\n",
      "Ep: 91 Bc: 168 | train_a=0.484 test_a=0.474 loss_a=1.563 | train_b=0.531 test_b=0.461 loss_b=1.581\n",
      "Ep: 92 Bc: 168 | train_a=0.422 test_a=0.489 loss_a=1.559 | train_b=0.438 test_b=0.488 loss_b=1.573\n",
      "Ep: 93 Bc: 168 | train_a=0.406 test_a=0.471 loss_a=1.576 | train_b=0.453 test_b=0.487 loss_b=1.583\n",
      "Ep: 94 Bc: 168 | train_a=0.484 test_a=0.477 loss_a=1.550 | train_b=0.438 test_b=0.478 loss_b=1.569\n",
      "Ep: 95 Bc: 168 | train_a=0.422 test_a=0.477 loss_a=1.563 | train_b=0.406 test_b=0.473 loss_b=1.566\n",
      "Ep: 96 Bc: 168 | train_a=0.516 test_a=0.502 loss_a=1.559 | train_b=0.531 test_b=0.483 loss_b=1.567\n",
      "Ep: 97 Bc: 168 | train_a=0.484 test_a=0.470 loss_a=1.553 | train_b=0.422 test_b=0.454 loss_b=1.572\n",
      "Ep: 98 Bc: 168 | train_a=0.500 test_a=0.472 loss_a=1.572 | train_b=0.500 test_b=0.463 loss_b=1.567\n",
      "Ep: 99 Bc: 168 | train_a=0.438 test_a=0.483 loss_a=1.572 | train_b=0.469 test_b=0.484 loss_b=1.579\n",
      "Ep:100 Bc: 168 | train_a=0.469 test_a=0.483 loss_a=1.562 | train_b=0.469 test_b=0.473 loss_b=1.576\n",
      "Process Time :68.72 s\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "# Training cycle\n",
    "all_test_x = t16_data.test.vectors_1\n",
    "all_test_y_t16_a = t16_data.test.labels_1\n",
    "all_test_y_t16_b = t16_data.test.labels_2\n",
    "start = time.time()\n",
    "for epoch_i in range(training_epochs):\n",
    "    ave_cost_a = 0\n",
    "    ave_cost_b = 0\n",
    "    for batch_i in range(total_batch):\n",
    "        batch_x, batch_y_t16_a, batch_y_t16_b = t16_data.train.next_batch(batch_size)\n",
    "        _, c_a = sess.run(\n",
    "            [optimizer_a, loss_a],\n",
    "            feed_dict={\n",
    "                X: batch_x,\n",
    "                Y_a: batch_y_t16_a,\n",
    "                keep_prob: train_dropout\n",
    "            })\n",
    "        _, c_b = sess.run(\n",
    "            [optimizer_b, loss_b],\n",
    "            feed_dict={\n",
    "                X: batch_x,\n",
    "                Y_b: batch_y_t16_b,\n",
    "                keep_prob: train_dropout\n",
    "            })\n",
    "        ave_cost_a += c_a / total_batch\n",
    "        ave_cost_b += c_b / total_batch\n",
    "    # Display logs per epoch step\n",
    "    if epoch_i % 1 == 0:\n",
    "        train_acc_a = sess.run(\n",
    "            accuracy_a,\n",
    "            feed_dict={\n",
    "                X: batch_x,\n",
    "                Y_a: batch_y_t16_a,\n",
    "                keep_prob: test_dropout\n",
    "            })\n",
    "        train_acc_b = sess.run(\n",
    "            accuracy_b,\n",
    "            feed_dict={\n",
    "                X: batch_x,\n",
    "                Y_b: batch_y_t16_b,\n",
    "                keep_prob: test_dropout\n",
    "            })\n",
    "        test_acc_a = sess.run(\n",
    "            accuracy_a,\n",
    "            feed_dict={\n",
    "                X: all_test_x,\n",
    "                Y_a: all_test_y_t16_a,\n",
    "                keep_prob: test_dropout\n",
    "            })\n",
    "        test_acc_b = sess.run(\n",
    "            accuracy_b,\n",
    "            feed_dict={\n",
    "                X: all_test_x,\n",
    "                Y_b: all_test_y_t16_b,\n",
    "                keep_prob: test_dropout\n",
    "            })\n",
    "        print(\"Ep:%3d Bc:%4d\" % (epoch_i + 1, batch_i + 1),\n",
    "              \"| train_a=%.3f\" % train_acc_a, \"test_a=%.3f\" % test_acc_a, \"loss_a=%5.3f\" % ave_cost_a,\n",
    "              \"| train_b=%.3f\" % train_acc_b, \"test_b=%.3f\" % test_acc_b, \"loss_b=%5.3f\" % ave_cost_b)\n",
    "end = time.time()\n",
    "print(\"Process Time :%.2f s\" % (end - start))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
