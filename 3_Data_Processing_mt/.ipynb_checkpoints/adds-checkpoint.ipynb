{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import MeCab\n",
    "import pickle\n",
    "import numpy as np\n",
    "import data_helpers as dh\n",
    "import pandas as pd\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import MDS,TSNE\n",
    "from collections import Counter\n",
    "from collections import OrderedDict\n",
    "from gensim.models import word2vec\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sen</th>\n",
       "      <th>sen_pre</th>\n",
       "      <th>say_id</th>\n",
       "      <th>reply_id</th>\n",
       "      <th>group_id</th>\n",
       "      <th>name</th>\n",
       "      <th>body</th>\n",
       "      <th>16types_a</th>\n",
       "      <th>16types_b</th>\n",
       "      <th>argument_a</th>\n",
       "      <th>argument_b</th>\n",
       "      <th>epistemic_a</th>\n",
       "      <th>epistemic_b</th>\n",
       "      <th>social_a</th>\n",
       "      <th>social_b</th>\n",
       "      <th>coordination_a</th>\n",
       "      <th>coordination_b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[よろしく, お願い, し, ます, ！, EOS]</td>\n",
       "      <td>[EOS]</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1234568</td>\n",
       "      <td>まこぴす</td>\n",
       "      <td>よろしくお願いします！</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[よろしく, お願い, し, ます, EOS]</td>\n",
       "      <td>[よろしく, お願い, し, ます, ！, EOS]</td>\n",
       "      <td>31</td>\n",
       "      <td>-1</td>\n",
       "      <td>1234568</td>\n",
       "      <td>哲</td>\n",
       "      <td>よろしくお願いします</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[名前, な, の, が, 恥ずかしい, です, EOS, よろしく, お願い, し, ます...</td>\n",
       "      <td>[よろしく, お願い, し, ます, EOS]</td>\n",
       "      <td>70</td>\n",
       "      <td>-1</td>\n",
       "      <td>1234568</td>\n",
       "      <td>仙波</td>\n",
       "      <td>名前なのが恥ずかしいです…\\nよろしくお願いします！</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 sen  \\\n",
       "0                         [よろしく, お願い, し, ます, ！, EOS]   \n",
       "1                            [よろしく, お願い, し, ます, EOS]   \n",
       "2  [名前, な, の, が, 恥ずかしい, です, EOS, よろしく, お願い, し, ます...   \n",
       "\n",
       "                      sen_pre say_id reply_id group_id  name  \\\n",
       "0                       [EOS]      1       -1  1234568  まこぴす   \n",
       "1  [よろしく, お願い, し, ます, ！, EOS]     31       -1  1234568     哲   \n",
       "2     [よろしく, お願い, し, ます, EOS]     70       -1  1234568    仙波   \n",
       "\n",
       "                         body 16types_a 16types_b argument_a argument_b  \\\n",
       "0                 よろしくお願いします！         5         5          1          1   \n",
       "1                  よろしくお願いします         5         5          1          1   \n",
       "2  名前なのが恥ずかしいです…\\nよろしくお願いします！         5         5          1          1   \n",
       "\n",
       "  epistemic_a epistemic_b social_a social_b coordination_a coordination_b  \n",
       "0           1           1        0        0              0              0  \n",
       "1           1           1        0        0              0              0  \n",
       "2           1           1        0        0              0              0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "All_df = pd.read_pickle(\"../data/all_mecab.pickle\")\n",
    "All_df.head(3)\n",
    "\n",
    "# senとsen_preの単語をIDに変換し、新たな列としてAll_dfに追加する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# senとsen_preの単語をIDに変換し、新たな列としてAll_dfに追加する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "単語: EOS \t出現数: 29580 \tID: 0\n",
      "単語: の \t出現数: 10567 \tID: 1\n",
      "単語: て \t出現数: 7408 \tID: 2\n",
      "単語: です \t出現数: 7390 \tID: 3\n",
      "単語: ます \t出現数: 7363 \tID: 4\n",
      "単語: か \t出現数: 7285 \tID: 5\n",
      "単語: 、 \t出現数: 6959 \tID: 6\n",
      "単語: に \t出現数: 6914 \tID: 7\n",
      "単語: が \t出現数: 6893 \tID: 8\n",
      "単語: は \t出現数: 6793 \tID: 9\n",
      "words kinds: 6961 words>=2: 6649\n"
     ]
    }
   ],
   "source": [
    "sen = All_df['sen'].values\n",
    "sen_pre = All_df['sen_pre'].values\n",
    "\n",
    "# 単語辞書の作成\n",
    "wd_set = Counter([x for s in (sen + sen_pre) for x in s])\n",
    "wd_ary = np.array(list(wd_set.keys()))\n",
    "wd_cnt = np.array(list(wd_set.values()))\n",
    "\n",
    "# 出現頻度順にソート\n",
    "wd_ary = wd_ary[np.argsort(wd_cnt)[::-1]]\n",
    "wd_cnt.sort()\n",
    "wd_cnt = wd_cnt[::-1]\n",
    "\n",
    "# 単語ID辞書の作成\n",
    "wd_to_id = {wd: i for i, wd in enumerate(wd_ary)}\n",
    "\n",
    "# Top10の単語を出力\n",
    "for i in range(10):\n",
    "    print(\"単語:\",\n",
    "          list(wd_ary)[i], \"\\t出現数:\",\n",
    "          list(wd_cnt)[i], \"\\tID:\", wd_to_id[list(wd_ary)[i]])\n",
    "\n",
    "# 出現数CUT_OFF以下の単語のIDを統一\n",
    "CUT_OFF = 2\n",
    "print(\"words kinds:\", len(wd_cnt), \"words>=\" + str(CUT_OFF) + \":\",\n",
    "      np.sum(wd_cnt >= CUT_OFF))\n",
    "other_id = np.sum(wd_cnt >= CUT_OFF)\n",
    "wd_to_id.update({wd: other_id for wd in wd_ary[wd_cnt < CUT_OFF]})\n",
    "id_to_wd = {wd_to_id[wd]: wd for wd in wd_to_id.keys()}\n",
    "\n",
    "# senとsen_preの単語をIDに変換\n",
    "sen_id = []\n",
    "sen_pre_id = []\n",
    "for s, s_pre in zip(sen, sen_pre):\n",
    "    sen_id.append([str(wd_to_id[wd]) for wd in s])\n",
    "    sen_pre_id.append([str(wd_to_id[wd]) for wd in s_pre])\n",
    "\n",
    "# 新し列としてAll_dfに追加\n",
    "All_df.insert(loc=0, column='sen_id', value=sen_id)\n",
    "All_df.insert(loc=1, column='sen_pre_id', value=sen_pre_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sen_id</th>\n",
       "      <th>sen_pre_id</th>\n",
       "      <th>sen</th>\n",
       "      <th>sen_pre</th>\n",
       "      <th>say_id</th>\n",
       "      <th>reply_id</th>\n",
       "      <th>group_id</th>\n",
       "      <th>name</th>\n",
       "      <th>body</th>\n",
       "      <th>16types_a</th>\n",
       "      <th>16types_b</th>\n",
       "      <th>argument_a</th>\n",
       "      <th>argument_b</th>\n",
       "      <th>epistemic_a</th>\n",
       "      <th>epistemic_b</th>\n",
       "      <th>social_a</th>\n",
       "      <th>social_b</th>\n",
       "      <th>coordination_a</th>\n",
       "      <th>coordination_b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[39, 35, 12, 4, 18, 0]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[よろしく, お願い, し, ます, ！, EOS]</td>\n",
       "      <td>[EOS]</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1234568</td>\n",
       "      <td>まこぴす</td>\n",
       "      <td>よろしくお願いします！</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[39, 35, 12, 4, 0]</td>\n",
       "      <td>[39, 35, 12, 4, 18, 0]</td>\n",
       "      <td>[よろしく, お願い, し, ます, EOS]</td>\n",
       "      <td>[よろしく, お願い, し, ます, ！, EOS]</td>\n",
       "      <td>31</td>\n",
       "      <td>-1</td>\n",
       "      <td>1234568</td>\n",
       "      <td>哲</td>\n",
       "      <td>よろしくお願いします</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[415, 19, 1, 8, 2253, 3, 0, 39, 35, 12, 4, 18, 0]</td>\n",
       "      <td>[39, 35, 12, 4, 0]</td>\n",
       "      <td>[名前, な, の, が, 恥ずかしい, です, EOS, よろしく, お願い, し, ます...</td>\n",
       "      <td>[よろしく, お願い, し, ます, EOS]</td>\n",
       "      <td>70</td>\n",
       "      <td>-1</td>\n",
       "      <td>1234568</td>\n",
       "      <td>仙波</td>\n",
       "      <td>名前なのが恥ずかしいです…\\nよろしくお願いします！</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sen_id              sen_pre_id  \\\n",
       "0                             [39, 35, 12, 4, 18, 0]                     [0]   \n",
       "1                                 [39, 35, 12, 4, 0]  [39, 35, 12, 4, 18, 0]   \n",
       "2  [415, 19, 1, 8, 2253, 3, 0, 39, 35, 12, 4, 18, 0]      [39, 35, 12, 4, 0]   \n",
       "\n",
       "                                                 sen  \\\n",
       "0                         [よろしく, お願い, し, ます, ！, EOS]   \n",
       "1                            [よろしく, お願い, し, ます, EOS]   \n",
       "2  [名前, な, の, が, 恥ずかしい, です, EOS, よろしく, お願い, し, ます...   \n",
       "\n",
       "                      sen_pre say_id reply_id group_id  name  \\\n",
       "0                       [EOS]      1       -1  1234568  まこぴす   \n",
       "1  [よろしく, お願い, し, ます, ！, EOS]     31       -1  1234568     哲   \n",
       "2     [よろしく, お願い, し, ます, EOS]     70       -1  1234568    仙波   \n",
       "\n",
       "                         body 16types_a 16types_b argument_a argument_b  \\\n",
       "0                 よろしくお願いします！         5         5          1          1   \n",
       "1                  よろしくお願いします         5         5          1          1   \n",
       "2  名前なのが恥ずかしいです…\\nよろしくお願いします！         5         5          1          1   \n",
       "\n",
       "  epistemic_a epistemic_b social_a social_b coordination_a coordination_b  \n",
       "0           1           1        0        0              0              0  \n",
       "1           1           1        0        0              0              0  \n",
       "2           1           1        0        0              0              0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "All_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec を利用し、単語のベクトル辞書を作成する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_sen_length: 292\n"
     ]
    }
   ],
   "source": [
    "sen_id = All_df['sen_id'].values\n",
    "sen_pre_id = All_df['sen_pre_id'].values\n",
    "sen_all = np.hstack((sen_id, sen_pre_id))\n",
    "\n",
    "max_sen_length = max([len(sen) for sen in sen_all])\n",
    "print(\"max_sen_length:\", max_sen_length)\n",
    "\n",
    "word_vectors_size = 200\n",
    "\n",
    "model = dh.get_w2v_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# （重要）各センテンスの長さを66に統一する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_sen_length: 66\n"
     ]
    }
   ],
   "source": [
    "All_df['sen_id'] = [x[:66] for x in All_df['sen_id']]\n",
    "All_df['sen_pre_id'] = [x[:66] for x in All_df['sen_pre_id']]\n",
    "\n",
    "sen_all = np.hstack((All_df['sen_id'].values, All_df['sen_pre_id'].values))\n",
    "max_sen_length = max([len(sen) for sen in sen_all])\n",
    "print(\"max_sen_length:\", max_sen_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データの準備\n",
    "* sentences dataをpaddingし、word vectorsによりfeature vectorsを作る\n",
    "* labels dataをone hotの型に変換する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "全データ(All_df)： (12012, 19)\n",
      "重複投稿を排除したデータ(All_drop_df)： (11357, 19) \n",
      "\n",
      "input data(sen)： (12012, 66, 200)\n",
      "input data(sen_pre)： (12012, 66, 200)\n"
     ]
    }
   ],
   "source": [
    "# データの整理（一致、重複）\n",
    "print(\"全データ(All_df)：\", All_df.shape)\n",
    "All_drop_df = All_df.drop_duplicates(subset=['body', 'name']).reset_index(drop=True)\n",
    "print(\"重複投稿を排除したデータ(All_drop_df)：\", All_drop_df.shape, \"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "all_sen = All_df['sen_id'].values\n",
    "all_sen = np.array([np.array(x, dtype=np.int32) for x in all_sen])\n",
    "x = dh.sen_to_fv(all_sen, max_sen_length, model, False)\n",
    "print(\"input data(sen)：\",x.shape)\n",
    "all_sen_pre = All_df['sen_pre_id'].values\n",
    "all_sen_pre = np.array([np.array(x, dtype=np.int32) for x in all_sen_pre])\n",
    "x_pre = dh.sen_to_fv(all_sen_pre, max_sen_length, model, False)\n",
    "print(\"input data(sen_pre)：\",x_pre.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input data(t16_a)： (12012, 16)\n",
      "input data(t16_b)： (12012, 16)\n"
     ]
    }
   ],
   "source": [
    "# 16types-------------------------------------\n",
    "# print(\"16types_a：\", Counter(All_df['16types_a']), \"\\n\")\n",
    "# print(\"16types_b：\", Counter(All_df['16types_b']), \"\\n\")\n",
    "\n",
    "label_t16_a = All_df['16types_a'].values\n",
    "label_t16_a = np.array(label_t16_a, dtype=np.int32)\n",
    "y_t16_a = dh.labels_to_one_hot(label_t16_a, 16)\n",
    "print(\"input data(t16_a)：\", y_t16_a.shape)\n",
    "label_t16_b = All_df['16types_b'].values\n",
    "label_t_b = np.array(label_t16_b, dtype=np.int32)\n",
    "y_t16_b = dh.labels_to_one_hot(label_t_b, 16)\n",
    "print(\"input data(t16_b)：\", y_t16_b.shape)\n",
    "\n",
    "t16_data = dh.set_data_sets(x, y_t16_a, y_t16_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168\n"
     ]
    }
   ],
   "source": [
    "# Network Parameters\n",
    "num_input = 200\n",
    "num_hidden = 200\n",
    "num_classes = 16\n",
    "train_dropout = 1.0\n",
    "test_dropout = 1.0\n",
    "embed_dim = word_vectors_size\n",
    "sents_len = max_sen_length\n",
    "\n",
    "\n",
    "# Training Parameters\n",
    "learning_rate = 0.01\n",
    "batch_size = 64\n",
    "total_batch = int(t16_data.train.num_examples / batch_size)\n",
    "print(total_batch)\n",
    "training_epochs = 100\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(dtype=tf.float32, shape=[None, sents_len, embed_dim])\n",
    "Y_a = tf.placeholder(dtype=tf.float32, shape=[None, num_classes])\n",
    "Y_b = tf.placeholder(dtype=tf.float32, shape=[None, num_classes])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.truncated_normal(stddev=0.1, shape=[num_input, num_hidden])),\n",
    "    'out': tf.Variable(tf.truncated_normal(stddev=0.1, shape=[num_hidden, num_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'h1': tf.Variable(tf.constant(value=0.1, shape=[num_hidden])),\n",
    "    'out': tf.Variable(tf.constant(value=0.1, shape=[num_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "def NN(x, weights, biases, dropout):\n",
    "\n",
    "    avg = tf.reduce_mean(x, axis=1) # [None, embed_dim]\n",
    "\n",
    "    h1 = tf.add(tf.matmul(avg, weights['h1']), biases['h1'])\n",
    "    h1_relu = tf.nn.relu(h1)\n",
    "    \n",
    "    h1_drop = tf.nn.dropout(h1_relu, dropout)\n",
    "    \n",
    "    out = tf.add(tf.matmul(h1_drop, weights['out']), biases['out'])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct model\n",
    "y_pred_a = NN(X, weights, biases, keep_prob)\n",
    "y_pred_b = NN(X, weights, biases, keep_prob)\n",
    "\n",
    "# y_softmax = tf.nn.softmax(y_pred)\n",
    "\n",
    "# Define loss and optimizer\n",
    "# type 1(old):\n",
    "# loss = tf.reduce_mean(\n",
    "#     -tf.reduce_sum(Y * tf.log(y_softmax), reduction_indices=[1]))\n",
    "# type 2(server):\n",
    "# loss = tf.reduce_mean(\n",
    "#     tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=y_pred))\n",
    "# type 3(new):\n",
    "loss_a = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y_a, logits=y_pred_a))\n",
    "\n",
    "loss_b = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y_b, logits=y_pred_b))\n",
    "\n",
    "joint_loss = loss_a + loss_b\n",
    "\n",
    "optimizer_a = tf.train.AdamOptimizer(learning_rate).minimize(loss_a)\n",
    "optimizer_b = tf.train.AdamOptimizer(learning_rate).minimize(loss_b)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(joint_loss)\n",
    "\n",
    "\n",
    "# Evaluate model\n",
    "pred_a = tf.argmax(y_pred_a, 1)\n",
    "true_a = tf.argmax(Y_a, 1)\n",
    "correct_prediction_a = tf.equal(pred_a, true_a)\n",
    "accuracy_a = tf.reduce_mean(tf.cast(correct_prediction_a, tf.float32))\n",
    "\n",
    "\n",
    "pred_b = tf.argmax(y_pred_b, 1)\n",
    "true_b = tf.argmax(Y_b, 1)\n",
    "correct_prediction_b = tf.equal(pred_b, true_b)\n",
    "accuracy_b = tf.reduce_mean(tf.cast(correct_prediction_b, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Batch: 168 train_acc_a=0.172 train_acc_b=0.266 test_acc_a=0.204 test_acc_b=0.225 train_cost=4.304\n",
      "Epoch: 2 Batch: 168 train_acc_a=0.344 train_acc_b=0.438 test_acc_a=0.394 test_acc_b=0.380 train_cost=3.956\n",
      "Epoch: 3 Batch: 168 train_acc_a=0.297 train_acc_b=0.391 test_acc_a=0.395 test_acc_b=0.383 train_cost=3.719\n",
      "Epoch: 4 Batch: 168 train_acc_a=0.406 train_acc_b=0.406 test_acc_a=0.401 test_acc_b=0.404 train_cost=3.635\n",
      "Epoch: 5 Batch: 168 train_acc_a=0.344 train_acc_b=0.469 test_acc_a=0.347 test_acc_b=0.347 train_cost=3.574\n",
      "Epoch: 6 Batch: 168 train_acc_a=0.375 train_acc_b=0.312 test_acc_a=0.418 test_acc_b=0.415 train_cost=3.564\n",
      "Epoch: 7 Batch: 168 train_acc_a=0.344 train_acc_b=0.484 test_acc_a=0.404 test_acc_b=0.394 train_cost=3.500\n",
      "Epoch: 8 Batch: 168 train_acc_a=0.469 train_acc_b=0.500 test_acc_a=0.440 test_acc_b=0.420 train_cost=3.432\n",
      "Epoch: 9 Batch: 168 train_acc_a=0.359 train_acc_b=0.422 test_acc_a=0.441 test_acc_b=0.427 train_cost=3.443\n",
      "Epoch:10 Batch: 168 train_acc_a=0.406 train_acc_b=0.375 test_acc_a=0.451 test_acc_b=0.413 train_cost=3.388\n",
      "Epoch:11 Batch: 168 train_acc_a=0.484 train_acc_b=0.469 test_acc_a=0.454 test_acc_b=0.423 train_cost=3.348\n",
      "Epoch:12 Batch: 168 train_acc_a=0.391 train_acc_b=0.422 test_acc_a=0.444 test_acc_b=0.416 train_cost=3.379\n",
      "Epoch:13 Batch: 168 train_acc_a=0.453 train_acc_b=0.438 test_acc_a=0.452 test_acc_b=0.436 train_cost=3.350\n",
      "Epoch:14 Batch: 168 train_acc_a=0.328 train_acc_b=0.391 test_acc_a=0.440 test_acc_b=0.423 train_cost=3.337\n",
      "Epoch:15 Batch: 168 train_acc_a=0.375 train_acc_b=0.391 test_acc_a=0.439 test_acc_b=0.450 train_cost=3.319\n",
      "Epoch:16 Batch: 168 train_acc_a=0.391 train_acc_b=0.469 test_acc_a=0.452 test_acc_b=0.471 train_cost=3.300\n",
      "Epoch:17 Batch: 168 train_acc_a=0.531 train_acc_b=0.516 test_acc_a=0.415 test_acc_b=0.431 train_cost=3.326\n",
      "Epoch:18 Batch: 168 train_acc_a=0.375 train_acc_b=0.484 test_acc_a=0.459 test_acc_b=0.438 train_cost=3.287\n",
      "Epoch:19 Batch: 168 train_acc_a=0.391 train_acc_b=0.453 test_acc_a=0.455 test_acc_b=0.462 train_cost=3.289\n",
      "Epoch:20 Batch: 168 train_acc_a=0.453 train_acc_b=0.484 test_acc_a=0.460 test_acc_b=0.479 train_cost=3.295\n",
      "Epoch:21 Batch: 168 train_acc_a=0.438 train_acc_b=0.438 test_acc_a=0.441 test_acc_b=0.426 train_cost=3.264\n",
      "Epoch:22 Batch: 168 train_acc_a=0.500 train_acc_b=0.500 test_acc_a=0.433 test_acc_b=0.433 train_cost=3.252\n",
      "Epoch:23 Batch: 168 train_acc_a=0.562 train_acc_b=0.516 test_acc_a=0.457 test_acc_b=0.464 train_cost=3.254\n",
      "Epoch:24 Batch: 168 train_acc_a=0.438 train_acc_b=0.406 test_acc_a=0.471 test_acc_b=0.454 train_cost=3.259\n",
      "Epoch:25 Batch: 168 train_acc_a=0.531 train_acc_b=0.516 test_acc_a=0.462 test_acc_b=0.452 train_cost=3.242\n",
      "Epoch:26 Batch: 168 train_acc_a=0.484 train_acc_b=0.516 test_acc_a=0.455 test_acc_b=0.474 train_cost=3.232\n",
      "Epoch:27 Batch: 168 train_acc_a=0.406 train_acc_b=0.453 test_acc_a=0.426 test_acc_b=0.441 train_cost=3.259\n",
      "Epoch:28 Batch: 168 train_acc_a=0.438 train_acc_b=0.438 test_acc_a=0.488 test_acc_b=0.462 train_cost=3.243\n",
      "Epoch:29 Batch: 168 train_acc_a=0.500 train_acc_b=0.484 test_acc_a=0.465 test_acc_b=0.448 train_cost=3.271\n",
      "Epoch:30 Batch: 168 train_acc_a=0.422 train_acc_b=0.469 test_acc_a=0.458 test_acc_b=0.462 train_cost=3.239\n",
      "Epoch:31 Batch: 168 train_acc_a=0.281 train_acc_b=0.312 test_acc_a=0.456 test_acc_b=0.460 train_cost=3.224\n",
      "Epoch:32 Batch: 168 train_acc_a=0.594 train_acc_b=0.562 test_acc_a=0.448 test_acc_b=0.446 train_cost=3.230\n",
      "Epoch:33 Batch: 168 train_acc_a=0.578 train_acc_b=0.531 test_acc_a=0.457 test_acc_b=0.476 train_cost=3.233\n",
      "Epoch:34 Batch: 168 train_acc_a=0.469 train_acc_b=0.453 test_acc_a=0.474 test_acc_b=0.465 train_cost=3.225\n",
      "Epoch:35 Batch: 168 train_acc_a=0.438 train_acc_b=0.422 test_acc_a=0.466 test_acc_b=0.452 train_cost=3.202\n",
      "Epoch:36 Batch: 168 train_acc_a=0.500 train_acc_b=0.484 test_acc_a=0.458 test_acc_b=0.474 train_cost=3.242\n",
      "Epoch:37 Batch: 168 train_acc_a=0.359 train_acc_b=0.297 test_acc_a=0.473 test_acc_b=0.475 train_cost=3.179\n",
      "Epoch:38 Batch: 168 train_acc_a=0.391 train_acc_b=0.375 test_acc_a=0.480 test_acc_b=0.455 train_cost=3.194\n",
      "Epoch:39 Batch: 168 train_acc_a=0.438 train_acc_b=0.500 test_acc_a=0.461 test_acc_b=0.475 train_cost=3.222\n",
      "Epoch:40 Batch: 168 train_acc_a=0.438 train_acc_b=0.422 test_acc_a=0.464 test_acc_b=0.485 train_cost=3.207\n",
      "Epoch:41 Batch: 168 train_acc_a=0.438 train_acc_b=0.469 test_acc_a=0.428 test_acc_b=0.432 train_cost=3.200\n",
      "Epoch:42 Batch: 168 train_acc_a=0.438 train_acc_b=0.500 test_acc_a=0.466 test_acc_b=0.446 train_cost=3.200\n",
      "Epoch:43 Batch: 168 train_acc_a=0.453 train_acc_b=0.438 test_acc_a=0.460 test_acc_b=0.464 train_cost=3.202\n",
      "Epoch:44 Batch: 168 train_acc_a=0.406 train_acc_b=0.469 test_acc_a=0.466 test_acc_b=0.479 train_cost=3.197\n",
      "Epoch:45 Batch: 168 train_acc_a=0.344 train_acc_b=0.453 test_acc_a=0.464 test_acc_b=0.477 train_cost=3.214\n",
      "Epoch:46 Batch: 168 train_acc_a=0.422 train_acc_b=0.453 test_acc_a=0.478 test_acc_b=0.484 train_cost=3.184\n",
      "Epoch:47 Batch: 168 train_acc_a=0.516 train_acc_b=0.531 test_acc_a=0.470 test_acc_b=0.464 train_cost=3.173\n",
      "Epoch:48 Batch: 168 train_acc_a=0.438 train_acc_b=0.438 test_acc_a=0.467 test_acc_b=0.447 train_cost=3.209\n",
      "Epoch:49 Batch: 168 train_acc_a=0.438 train_acc_b=0.469 test_acc_a=0.453 test_acc_b=0.442 train_cost=3.172\n",
      "Epoch:50 Batch: 168 train_acc_a=0.609 train_acc_b=0.484 test_acc_a=0.481 test_acc_b=0.461 train_cost=3.184\n",
      "Epoch:51 Batch: 168 train_acc_a=0.484 train_acc_b=0.438 test_acc_a=0.469 test_acc_b=0.436 train_cost=3.175\n",
      "Epoch:52 Batch: 168 train_acc_a=0.391 train_acc_b=0.375 test_acc_a=0.458 test_acc_b=0.418 train_cost=3.182\n",
      "Epoch:53 Batch: 168 train_acc_a=0.328 train_acc_b=0.328 test_acc_a=0.495 test_acc_b=0.462 train_cost=3.170\n",
      "Epoch:54 Batch: 168 train_acc_a=0.516 train_acc_b=0.547 test_acc_a=0.479 test_acc_b=0.481 train_cost=3.191\n",
      "Epoch:55 Batch: 168 train_acc_a=0.469 train_acc_b=0.422 test_acc_a=0.471 test_acc_b=0.478 train_cost=3.165\n",
      "Epoch:56 Batch: 168 train_acc_a=0.453 train_acc_b=0.531 test_acc_a=0.471 test_acc_b=0.453 train_cost=3.167\n",
      "Epoch:57 Batch: 168 train_acc_a=0.438 train_acc_b=0.484 test_acc_a=0.477 test_acc_b=0.483 train_cost=3.166\n",
      "Epoch:58 Batch: 168 train_acc_a=0.531 train_acc_b=0.500 test_acc_a=0.485 test_acc_b=0.478 train_cost=3.176\n",
      "Epoch:59 Batch: 168 train_acc_a=0.516 train_acc_b=0.547 test_acc_a=0.467 test_acc_b=0.466 train_cost=3.152\n",
      "Epoch:60 Batch: 168 train_acc_a=0.484 train_acc_b=0.422 test_acc_a=0.464 test_acc_b=0.466 train_cost=3.143\n",
      "Epoch:61 Batch: 168 train_acc_a=0.484 train_acc_b=0.422 test_acc_a=0.481 test_acc_b=0.458 train_cost=3.140\n",
      "Epoch:62 Batch: 168 train_acc_a=0.594 train_acc_b=0.547 test_acc_a=0.488 test_acc_b=0.463 train_cost=3.164\n",
      "Epoch:63 Batch: 168 train_acc_a=0.438 train_acc_b=0.406 test_acc_a=0.481 test_acc_b=0.451 train_cost=3.165\n",
      "Epoch:64 Batch: 168 train_acc_a=0.562 train_acc_b=0.531 test_acc_a=0.488 test_acc_b=0.466 train_cost=3.161\n",
      "Epoch:65 Batch: 168 train_acc_a=0.531 train_acc_b=0.484 test_acc_a=0.486 test_acc_b=0.478 train_cost=3.139\n",
      "Epoch:66 Batch: 168 train_acc_a=0.531 train_acc_b=0.516 test_acc_a=0.478 test_acc_b=0.476 train_cost=3.165\n",
      "Epoch:67 Batch: 168 train_acc_a=0.438 train_acc_b=0.406 test_acc_a=0.474 test_acc_b=0.457 train_cost=3.129\n",
      "Epoch:68 Batch: 168 train_acc_a=0.500 train_acc_b=0.500 test_acc_a=0.474 test_acc_b=0.478 train_cost=3.152\n",
      "Epoch:69 Batch: 168 train_acc_a=0.500 train_acc_b=0.562 test_acc_a=0.463 test_acc_b=0.468 train_cost=3.139\n",
      "Epoch:70 Batch: 168 train_acc_a=0.516 train_acc_b=0.641 test_acc_a=0.474 test_acc_b=0.471 train_cost=3.135\n",
      "Epoch:71 Batch: 168 train_acc_a=0.453 train_acc_b=0.422 test_acc_a=0.487 test_acc_b=0.475 train_cost=3.147\n",
      "Epoch:72 Batch: 168 train_acc_a=0.469 train_acc_b=0.531 test_acc_a=0.466 test_acc_b=0.470 train_cost=3.126\n",
      "Epoch:73 Batch: 168 train_acc_a=0.438 train_acc_b=0.641 test_acc_a=0.457 test_acc_b=0.477 train_cost=3.152\n",
      "Epoch:74 Batch: 168 train_acc_a=0.438 train_acc_b=0.422 test_acc_a=0.477 test_acc_b=0.463 train_cost=3.116\n",
      "Epoch:75 Batch: 168 train_acc_a=0.484 train_acc_b=0.531 test_acc_a=0.458 test_acc_b=0.463 train_cost=3.150\n",
      "Epoch:76 Batch: 168 train_acc_a=0.453 train_acc_b=0.422 test_acc_a=0.498 test_acc_b=0.467 train_cost=3.125\n",
      "Epoch:77 Batch: 168 train_acc_a=0.547 train_acc_b=0.500 test_acc_a=0.477 test_acc_b=0.466 train_cost=3.124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:78 Batch: 168 train_acc_a=0.391 train_acc_b=0.469 test_acc_a=0.490 test_acc_b=0.462 train_cost=3.129\n",
      "Epoch:79 Batch: 168 train_acc_a=0.469 train_acc_b=0.484 test_acc_a=0.470 test_acc_b=0.445 train_cost=3.133\n",
      "Epoch:80 Batch: 168 train_acc_a=0.500 train_acc_b=0.547 test_acc_a=0.482 test_acc_b=0.467 train_cost=3.110\n",
      "Epoch:81 Batch: 168 train_acc_a=0.484 train_acc_b=0.438 test_acc_a=0.488 test_acc_b=0.453 train_cost=3.098\n",
      "Epoch:82 Batch: 168 train_acc_a=0.500 train_acc_b=0.375 test_acc_a=0.444 test_acc_b=0.427 train_cost=3.118\n",
      "Epoch:83 Batch: 168 train_acc_a=0.453 train_acc_b=0.359 test_acc_a=0.467 test_acc_b=0.482 train_cost=3.104\n",
      "Epoch:84 Batch: 168 train_acc_a=0.453 train_acc_b=0.422 test_acc_a=0.477 test_acc_b=0.449 train_cost=3.127\n",
      "Epoch:85 Batch: 168 train_acc_a=0.281 train_acc_b=0.328 test_acc_a=0.491 test_acc_b=0.469 train_cost=3.145\n",
      "Epoch:86 Batch: 168 train_acc_a=0.500 train_acc_b=0.531 test_acc_a=0.425 test_acc_b=0.447 train_cost=3.134\n",
      "Epoch:87 Batch: 168 train_acc_a=0.516 train_acc_b=0.516 test_acc_a=0.467 test_acc_b=0.471 train_cost=3.131\n",
      "Epoch:88 Batch: 168 train_acc_a=0.531 train_acc_b=0.422 test_acc_a=0.491 test_acc_b=0.473 train_cost=3.111\n",
      "Epoch:89 Batch: 168 train_acc_a=0.547 train_acc_b=0.578 test_acc_a=0.472 test_acc_b=0.472 train_cost=3.098\n",
      "Epoch:90 Batch: 168 train_acc_a=0.484 train_acc_b=0.547 test_acc_a=0.490 test_acc_b=0.458 train_cost=3.100\n",
      "Epoch:91 Batch: 168 train_acc_a=0.500 train_acc_b=0.453 test_acc_a=0.456 test_acc_b=0.458 train_cost=3.114\n",
      "Epoch:92 Batch: 168 train_acc_a=0.500 train_acc_b=0.516 test_acc_a=0.453 test_acc_b=0.467 train_cost=3.135\n",
      "Epoch:93 Batch: 168 train_acc_a=0.484 train_acc_b=0.516 test_acc_a=0.483 test_acc_b=0.474 train_cost=3.092\n",
      "Epoch:94 Batch: 168 train_acc_a=0.484 train_acc_b=0.516 test_acc_a=0.462 test_acc_b=0.474 train_cost=3.096\n",
      "Epoch:95 Batch: 168 train_acc_a=0.438 train_acc_b=0.516 test_acc_a=0.488 test_acc_b=0.483 train_cost=3.100\n",
      "Epoch:96 Batch: 168 train_acc_a=0.359 train_acc_b=0.422 test_acc_a=0.410 test_acc_b=0.440 train_cost=3.117\n",
      "Epoch:97 Batch: 168 train_acc_a=0.484 train_acc_b=0.547 test_acc_a=0.458 test_acc_b=0.475 train_cost=3.091\n",
      "Epoch:98 Batch: 168 train_acc_a=0.469 train_acc_b=0.406 test_acc_a=0.482 test_acc_b=0.454 train_cost=3.102\n",
      "Epoch:99 Batch: 168 train_acc_a=0.562 train_acc_b=0.578 test_acc_a=0.468 test_acc_b=0.482 train_cost=3.104\n",
      "Epoch:100 Batch: 168 train_acc_a=0.531 train_acc_b=0.516 test_acc_a=0.485 test_acc_b=0.458 train_cost=3.107\n",
      "Process Time :53.58 s\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "# Training cycle\n",
    "all_test_x = t16_data.test.vectors\n",
    "all_test_y_t16_a = t16_data.test.labels_t16_a\n",
    "all_test_y_t16_b = t16_data.test.labels_t16_b\n",
    "start = time.time()\n",
    "for epoch_i in range(training_epochs):\n",
    "    ave_cost = 0\n",
    "    for batch_i in range(total_batch):\n",
    "        batch_x, batch_y_t16_a, batch_y_t16_b = t16_data.train.next_batch(batch_size)\n",
    "        _, c = sess.run(\n",
    "            [optimizer, joint_loss],\n",
    "            feed_dict={\n",
    "                X: batch_x,\n",
    "                Y_a: batch_y_t16_a,\n",
    "                Y_b: batch_y_t16_b,\n",
    "                keep_prob: train_dropout\n",
    "            })\n",
    "        ave_cost += c / total_batch\n",
    "    # Display logs per epoch step\n",
    "    if epoch_i % 1 == 0:\n",
    "        train_acc_a = sess.run(\n",
    "            accuracy_a,\n",
    "            feed_dict={\n",
    "                X: batch_x,\n",
    "                Y_a: batch_y_t16_a,\n",
    "                keep_prob: test_dropout\n",
    "            })\n",
    "        train_acc_b = sess.run(\n",
    "            accuracy_b,\n",
    "            feed_dict={\n",
    "                X: batch_x,\n",
    "                Y_b: batch_y_t16_b,\n",
    "                keep_prob: test_dropout\n",
    "            })\n",
    "        test_acc_a = sess.run(\n",
    "            accuracy_a,\n",
    "            feed_dict={\n",
    "                X: all_test_x,\n",
    "                Y_a: all_test_y_t16_a,\n",
    "                keep_prob: test_dropout\n",
    "            })\n",
    "        test_acc_b = sess.run(\n",
    "            accuracy_b,\n",
    "            feed_dict={\n",
    "                X: all_test_x,\n",
    "                Y_b: all_test_y_t16_b,\n",
    "                keep_prob: test_dropout\n",
    "            })\n",
    "        print(\"Epoch:%2d Batch:%4d\" % (epoch_i + 1, batch_i + 1),\n",
    "              \"train_acc_a=%.3f\" % train_acc_a, \"train_acc_b=%.3f\" % train_acc_b,\n",
    "              \"test_acc_a=%.3f\" % test_acc_a, \"test_acc_b=%.3f\" % test_acc_b,\n",
    "              \"train_cost=%5.3f\" % ave_cost)\n",
    "end = time.time()\n",
    "print(\"Process Time :%.2f s\" % (end - start))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
