{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import MeCab\n",
    "import pickle\n",
    "import numpy as np\n",
    "import data_helpers as dh\n",
    "import pandas as pd\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import MDS,TSNE\n",
    "from collections import Counter\n",
    "from collections import OrderedDict\n",
    "from gensim.models import word2vec\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sen</th>\n",
       "      <th>sen_pre</th>\n",
       "      <th>say_id</th>\n",
       "      <th>reply_id</th>\n",
       "      <th>group_id</th>\n",
       "      <th>name</th>\n",
       "      <th>body</th>\n",
       "      <th>16types_a</th>\n",
       "      <th>16types_b</th>\n",
       "      <th>argument_a</th>\n",
       "      <th>argument_b</th>\n",
       "      <th>epistemic_a</th>\n",
       "      <th>epistemic_b</th>\n",
       "      <th>social_a</th>\n",
       "      <th>social_b</th>\n",
       "      <th>coordination_a</th>\n",
       "      <th>coordination_b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[よろしく, お願い, し, ます, ！, EOS]</td>\n",
       "      <td>[EOS]</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1234568</td>\n",
       "      <td>まこぴす</td>\n",
       "      <td>よろしくお願いします！</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[よろしく, お願い, し, ます, EOS]</td>\n",
       "      <td>[よろしく, お願い, し, ます, ！, EOS]</td>\n",
       "      <td>31</td>\n",
       "      <td>-1</td>\n",
       "      <td>1234568</td>\n",
       "      <td>哲</td>\n",
       "      <td>よろしくお願いします</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[名前, な, の, が, 恥ずかしい, です, EOS, よろしく, お願い, し, ます...</td>\n",
       "      <td>[よろしく, お願い, し, ます, EOS]</td>\n",
       "      <td>70</td>\n",
       "      <td>-1</td>\n",
       "      <td>1234568</td>\n",
       "      <td>仙波</td>\n",
       "      <td>名前なのが恥ずかしいです…\\nよろしくお願いします！</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 sen  \\\n",
       "0                         [よろしく, お願い, し, ます, ！, EOS]   \n",
       "1                            [よろしく, お願い, し, ます, EOS]   \n",
       "2  [名前, な, の, が, 恥ずかしい, です, EOS, よろしく, お願い, し, ます...   \n",
       "\n",
       "                      sen_pre say_id reply_id group_id  name  \\\n",
       "0                       [EOS]      1       -1  1234568  まこぴす   \n",
       "1  [よろしく, お願い, し, ます, ！, EOS]     31       -1  1234568     哲   \n",
       "2     [よろしく, お願い, し, ます, EOS]     70       -1  1234568    仙波   \n",
       "\n",
       "                         body 16types_a 16types_b argument_a argument_b  \\\n",
       "0                 よろしくお願いします！         5         5          1          1   \n",
       "1                  よろしくお願いします         5         5          1          1   \n",
       "2  名前なのが恥ずかしいです…\\nよろしくお願いします！         5         5          1          1   \n",
       "\n",
       "  epistemic_a epistemic_b social_a social_b coordination_a coordination_b  \n",
       "0           1           1        0        0              0              0  \n",
       "1           1           1        0        0              0              0  \n",
       "2           1           1        0        0              0              0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "All_df = pd.read_pickle(\"../data/all_mecab.pickle\")\n",
    "All_df.head(3)\n",
    "\n",
    "# senとsen_preの単語をIDに変換し、新たな列としてAll_dfに追加する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# senとsen_preの単語をIDに変換し、新たな列としてAll_dfに追加する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "単語: EOS \t出現数: 29580 \tID: 0\n",
      "単語: の \t出現数: 10567 \tID: 1\n",
      "単語: て \t出現数: 7408 \tID: 2\n",
      "単語: です \t出現数: 7390 \tID: 3\n",
      "単語: ます \t出現数: 7363 \tID: 4\n",
      "単語: か \t出現数: 7285 \tID: 5\n",
      "単語: 、 \t出現数: 6959 \tID: 6\n",
      "単語: に \t出現数: 6914 \tID: 7\n",
      "単語: が \t出現数: 6893 \tID: 8\n",
      "単語: は \t出現数: 6793 \tID: 9\n",
      "words kinds: 6961 words>=2: 6649\n"
     ]
    }
   ],
   "source": [
    "sen = All_df['sen'].values\n",
    "sen_pre = All_df['sen_pre'].values\n",
    "\n",
    "# 単語辞書の作成\n",
    "wd_set = Counter([x for s in (sen + sen_pre) for x in s])\n",
    "wd_ary = np.array(list(wd_set.keys()))\n",
    "wd_cnt = np.array(list(wd_set.values()))\n",
    "\n",
    "# 出現頻度順にソート\n",
    "wd_ary = wd_ary[np.argsort(wd_cnt)[::-1]]\n",
    "wd_cnt.sort()\n",
    "wd_cnt = wd_cnt[::-1]\n",
    "\n",
    "# 単語ID辞書の作成\n",
    "wd_to_id = {wd: i for i, wd in enumerate(wd_ary)}\n",
    "\n",
    "# Top10の単語を出力\n",
    "for i in range(10):\n",
    "    print(\"単語:\",\n",
    "          list(wd_ary)[i], \"\\t出現数:\",\n",
    "          list(wd_cnt)[i], \"\\tID:\", wd_to_id[list(wd_ary)[i]])\n",
    "\n",
    "# 出現数CUT_OFF以下の単語のIDを統一\n",
    "CUT_OFF = 2\n",
    "print(\"words kinds:\", len(wd_cnt), \"words>=\" + str(CUT_OFF) + \":\",\n",
    "      np.sum(wd_cnt >= CUT_OFF))\n",
    "other_id = np.sum(wd_cnt >= CUT_OFF)\n",
    "wd_to_id.update({wd: other_id for wd in wd_ary[wd_cnt < CUT_OFF]})\n",
    "id_to_wd = {wd_to_id[wd]: wd for wd in wd_to_id.keys()}\n",
    "\n",
    "# senとsen_preの単語をIDに変換\n",
    "sen_id = []\n",
    "sen_pre_id = []\n",
    "for s, s_pre in zip(sen, sen_pre):\n",
    "    sen_id.append([str(wd_to_id[wd]) for wd in s])\n",
    "    sen_pre_id.append([str(wd_to_id[wd]) for wd in s_pre])\n",
    "\n",
    "# 新し列としてAll_dfに追加\n",
    "All_df.insert(loc=0, column='sen_id', value=sen_id)\n",
    "All_df.insert(loc=1, column='sen_pre_id', value=sen_pre_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sen_id</th>\n",
       "      <th>sen_pre_id</th>\n",
       "      <th>sen</th>\n",
       "      <th>sen_pre</th>\n",
       "      <th>say_id</th>\n",
       "      <th>reply_id</th>\n",
       "      <th>group_id</th>\n",
       "      <th>name</th>\n",
       "      <th>body</th>\n",
       "      <th>16types_a</th>\n",
       "      <th>16types_b</th>\n",
       "      <th>argument_a</th>\n",
       "      <th>argument_b</th>\n",
       "      <th>epistemic_a</th>\n",
       "      <th>epistemic_b</th>\n",
       "      <th>social_a</th>\n",
       "      <th>social_b</th>\n",
       "      <th>coordination_a</th>\n",
       "      <th>coordination_b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[39, 35, 12, 4, 18, 0]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[よろしく, お願い, し, ます, ！, EOS]</td>\n",
       "      <td>[EOS]</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1234568</td>\n",
       "      <td>まこぴす</td>\n",
       "      <td>よろしくお願いします！</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[39, 35, 12, 4, 0]</td>\n",
       "      <td>[39, 35, 12, 4, 18, 0]</td>\n",
       "      <td>[よろしく, お願い, し, ます, EOS]</td>\n",
       "      <td>[よろしく, お願い, し, ます, ！, EOS]</td>\n",
       "      <td>31</td>\n",
       "      <td>-1</td>\n",
       "      <td>1234568</td>\n",
       "      <td>哲</td>\n",
       "      <td>よろしくお願いします</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[413, 19, 1, 8, 2309, 3, 0, 39, 35, 12, 4, 18, 0]</td>\n",
       "      <td>[39, 35, 12, 4, 0]</td>\n",
       "      <td>[名前, な, の, が, 恥ずかしい, です, EOS, よろしく, お願い, し, ます...</td>\n",
       "      <td>[よろしく, お願い, し, ます, EOS]</td>\n",
       "      <td>70</td>\n",
       "      <td>-1</td>\n",
       "      <td>1234568</td>\n",
       "      <td>仙波</td>\n",
       "      <td>名前なのが恥ずかしいです…\\nよろしくお願いします！</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sen_id              sen_pre_id  \\\n",
       "0                             [39, 35, 12, 4, 18, 0]                     [0]   \n",
       "1                                 [39, 35, 12, 4, 0]  [39, 35, 12, 4, 18, 0]   \n",
       "2  [413, 19, 1, 8, 2309, 3, 0, 39, 35, 12, 4, 18, 0]      [39, 35, 12, 4, 0]   \n",
       "\n",
       "                                                 sen  \\\n",
       "0                         [よろしく, お願い, し, ます, ！, EOS]   \n",
       "1                            [よろしく, お願い, し, ます, EOS]   \n",
       "2  [名前, な, の, が, 恥ずかしい, です, EOS, よろしく, お願い, し, ます...   \n",
       "\n",
       "                      sen_pre say_id reply_id group_id  name  \\\n",
       "0                       [EOS]      1       -1  1234568  まこぴす   \n",
       "1  [よろしく, お願い, し, ます, ！, EOS]     31       -1  1234568     哲   \n",
       "2     [よろしく, お願い, し, ます, EOS]     70       -1  1234568    仙波   \n",
       "\n",
       "                         body 16types_a 16types_b argument_a argument_b  \\\n",
       "0                 よろしくお願いします！         5         5          1          1   \n",
       "1                  よろしくお願いします         5         5          1          1   \n",
       "2  名前なのが恥ずかしいです…\\nよろしくお願いします！         5         5          1          1   \n",
       "\n",
       "  epistemic_a epistemic_b social_a social_b coordination_a coordination_b  \n",
       "0           1           1        0        0              0              0  \n",
       "1           1           1        0        0              0              0  \n",
       "2           1           1        0        0              0              0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "All_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec を利用し、単語のベクトル辞書を作成する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_sen_length: 292\n"
     ]
    }
   ],
   "source": [
    "sen_id = All_df['sen_id'].values\n",
    "sen_pre_id = All_df['sen_pre_id'].values\n",
    "sen_all = np.hstack((sen_id, sen_pre_id))\n",
    "\n",
    "max_sen_length = max([len(sen) for sen in sen_all])\n",
    "print(\"max_sen_length:\", max_sen_length)\n",
    "\n",
    "word_vectors_size = 200\n",
    "\n",
    "model = dh.get_w2v_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# （重要）各センテンスの長さを66に統一する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_sen_length: 66\n"
     ]
    }
   ],
   "source": [
    "All_df['sen_id'] = [x[:66] for x in All_df['sen_id']]\n",
    "All_df['sen_pre_id'] = [x[:66] for x in All_df['sen_pre_id']]\n",
    "\n",
    "sen_all = np.hstack((All_df['sen_id'].values, All_df['sen_pre_id'].values))\n",
    "max_sen_length = max([len(sen) for sen in sen_all])\n",
    "print(\"max_sen_length:\", max_sen_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データの準備\n",
    "* sentences dataをpaddingし、word vectorsによりfeature vectorsを作る\n",
    "* labels dataをone hotの型に変換する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "全データ(All_df)： (12012, 19)\n",
      "重複投稿を排除したデータ(All_drop_df)： (11357, 19) \n",
      "\n",
      "input data(sen)： (11357, 66, 200)\n",
      "input data(sen_pre)： (11357, 66, 200)\n"
     ]
    }
   ],
   "source": [
    "# データの整理（一致、重複）\n",
    "print(\"全データ(All_df)：\", All_df.shape)\n",
    "All_drop_df = All_df.drop_duplicates(subset=['body', 'name']).reset_index(drop=True)\n",
    "print(\"重複投稿を排除したデータ(All_drop_df)：\", All_drop_df.shape, \"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "all_sen = All_drop_df['sen_id'].values\n",
    "all_sen = np.array([np.array(x, dtype=np.int32) for x in all_sen])\n",
    "x = dh.sen_to_fv(all_sen, max_sen_length, model, False)\n",
    "print(\"input data(sen)：\",x.shape)\n",
    "all_sen_pre = All_drop_df['sen_pre_id'].values\n",
    "all_sen_pre = np.array([np.array(x, dtype=np.int32) for x in all_sen_pre])\n",
    "x_pre = dh.sen_to_fv(all_sen_pre, max_sen_length, model, False)\n",
    "print(\"input data(sen_pre)：\",x_pre.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16types_a： Counter({1: 2425, 2: 1938, 6: 1307, 3: 1301, 4: 1230, 5: 1224, 8: 550, 9: 510, 7: 393, 14: 310, 15: 218, 11: 184, 0: 143, 10: 123, 12: 100, 13: 56}) \n",
      "\n",
      "input data(t16_a)： (11357, 16)\n",
      "input data(arg_a)： (11357, 6)\n",
      "input data(epi_a)： (11357, 4)\n",
      "(1136, 16)\n",
      "(1136, 6)\n",
      "(1136, 4)\n"
     ]
    }
   ],
   "source": [
    "# 16types-------------------------------------\n",
    "print(\"16types_a：\", Counter(All_df['16types_a']), \"\\n\")\n",
    "\n",
    "label_t16_a = All_drop_df['16types_a'].values\n",
    "label_t16_a = np.array(label_t16_a, dtype=np.int32)\n",
    "y_t16_a = dh.labels_to_one_hot(label_t16_a, 16)\n",
    "print(\"input data(t16_a)：\", y_t16_a.shape)\n",
    "\n",
    "label_arg_a = All_drop_df['argument_a'].values\n",
    "label_arg_a = np.array(label_arg_a, dtype=np.int32)\n",
    "y_arg_a = dh.labels_to_one_hot(label_arg_a, 6)\n",
    "print(\"input data(arg_a)：\", y_arg_a.shape)\n",
    "\n",
    "label_epi_a = All_drop_df['epistemic_a'].values\n",
    "label_epi_a = np.array(label_epi_a, dtype=np.int32)\n",
    "y_epi_a = dh.labels_to_one_hot(label_epi_a, 4)\n",
    "print(\"input data(epi_a)：\", y_epi_a.shape)\n",
    "\n",
    "\n",
    "data = dh.set_data_sets_3(x, x_pre, y_t16_a, y_arg_a, y_epi_a)\n",
    "\n",
    "\n",
    "print(data.test.labels_1.shape)\n",
    "print(data.test.labels_2.shape)\n",
    "print(data.test.labels_3.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_batch: 159\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = word_vectors_size\n",
    "num_classes_1 = 16\n",
    "num_classes_2 = 6\n",
    "num_classes_3 = 4\n",
    "sequence_length = max_sen_length\n",
    "num_steps = max_sen_length\n",
    "num_lstm_hidden = 200\n",
    "num_hidden = 200\n",
    "\n",
    "train_dropout = 0.8\n",
    "test_dropout = 1.0\n",
    "\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "total_batch = int(data.train.num_examples / batch_size)\n",
    "print(\"total_batch:\", total_batch)\n",
    "training_epochs = 40\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(dtype=tf.float32, shape=[None, sequence_length, embedding_dim])\n",
    "X_pre = tf.placeholder(dtype=tf.float32, shape=[None, sequence_length, embedding_dim])\n",
    "Y_1 = tf.placeholder(dtype=tf.float32, shape=[None, num_classes_1])\n",
    "Y_2 = tf.placeholder(dtype=tf.float32, shape=[None, num_classes_2])\n",
    "Y_3 = tf.placeholder(dtype=tf.float32, shape=[None, num_classes_3])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'fc1':tf.Variable(tf.truncated_normal(stddev=0.1, shape=[2*num_lstm_hidden, num_hidden])),\n",
    "    'out_1':tf.Variable(tf.truncated_normal(stddev=0.1, shape=[num_hidden, num_classes_1])),\n",
    "    'out_2':tf.Variable(tf.truncated_normal(stddev=0.1, shape=[num_hidden, num_classes_2])),\n",
    "    'out_3':tf.Variable(tf.truncated_normal(stddev=0.1, shape=[num_hidden, num_classes_3]))\n",
    "}\n",
    "biases = {\n",
    "    'fc1': tf.Variable(tf.constant(value=0.1, shape=[num_hidden])),\n",
    "    'out_1': tf.Variable(tf.constant(value=0.1, shape=[num_classes_1])),\n",
    "    'out_2': tf.Variable(tf.constant(value=0.1, shape=[num_classes_2])),\n",
    "    'out_3': tf.Variable(tf.constant(value=0.1, shape=[num_classes_3]))\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Si_S2S(x, x_pre, weights, biases, dropout):\n",
    "\n",
    "    num_layers = 2\n",
    "    \n",
    "    with tf.variable_scope('encoder'):\n",
    "        enc_cell = tf.nn.rnn_cell.BasicLSTMCell(num_lstm_hidden)\n",
    "        multi_enc_cell = tf.nn.rnn_cell.MultiRNNCell([enc_cell] * num_layers)\n",
    "        enc_outputs, enc_states = tf.nn.dynamic_rnn(multi_enc_cell, x, dtype=tf.float32)\n",
    "        enc_output = tf.concat(enc_outputs, axis=2)\n",
    "    with tf.variable_scope('decoder'):\n",
    "        dec_cell = tf.nn.rnn_cell.BasicLSTMCell(num_lstm_hidden)\n",
    "        multi_dec_cell = tf.nn.rnn_cell.MultiRNNCell([dec_cell] * num_layers)\n",
    "        dec_outputs, dec_states = tf.nn.dynamic_rnn(multi_dec_cell, x_pre, initial_state=enc_states)\n",
    "        dec_output = tf.concat(dec_outputs, axis=2)\n",
    "    \n",
    "    \n",
    "\n",
    "    # Full connection layer\n",
    "    enc_out = tf.reduce_mean(enc_output, axis=1)\n",
    "    dec_out = tf.reduce_mean(dec_output, axis=1)\n",
    "    s2s_out = tf.concat([enc_out, dec_out], axis=1)\n",
    "    \n",
    "    fc1 = tf.add(tf.matmul(s2s_out, weights['fc1']), biases['fc1'])\n",
    "    fc1_relu = tf.nn.relu(fc1)\n",
    "    fc1_drop = tf.nn.dropout(fc1_relu, dropout)\n",
    "    \n",
    "    out_1 = tf.add(tf.matmul(fc1_drop, weights['out_1']), biases['out_1'])\n",
    "    out_2 = tf.add(tf.matmul(fc1_drop, weights['out_2']), biases['out_2'])\n",
    "    out_3 = tf.add(tf.matmul(fc1_drop, weights['out_3']), biases['out_3'])\n",
    "    return out_1, out_2, out_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct model\n",
    "y_pred_1, y_pred_2, y_pred_3 = Si_S2S(X, X_pre, weights, biases, keep_prob)\n",
    "\n",
    "# y_softmax = tf.nn.softmax(y_pred)\n",
    "\n",
    "# Define loss and optimizer\n",
    "# type 1(old):\n",
    "# loss = tf.reduce_mean(\n",
    "#     -tf.reduce_sum(Y * tf.log(y_softmax), reduction_indices=[1]))\n",
    "# type 2(server):\n",
    "# loss = tf.reduce_mean(\n",
    "#     tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=y_pred))\n",
    "# type 3(new):\n",
    "# loss = tf.reduce_mean(\n",
    "#     tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=y_pred))\n",
    "\n",
    "\n",
    "loss_1 = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=Y_1, logits=y_pred_1))\n",
    "\n",
    "loss_2 = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=Y_2, logits=y_pred_2))\n",
    "\n",
    "loss_3 = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=Y_3, logits=y_pred_3))\n",
    "\n",
    "\n",
    "joint_loss = loss_1 + loss_2 + loss_3\n",
    "\n",
    "optimizer_1 = tf.train.AdamOptimizer(learning_rate).minimize(loss_1)\n",
    "optimizer_2 = tf.train.AdamOptimizer(learning_rate).minimize(loss_2)\n",
    "optimizer_3 = tf.train.AdamOptimizer(learning_rate).minimize(loss_3)\n",
    "\n",
    "optimizer_joint = tf.train.AdamOptimizer(learning_rate).minimize(joint_loss)\n",
    "\n",
    "# Evaluate model\n",
    "pred_1 = tf.argmax(y_pred_1, 1)\n",
    "true_1 = tf.argmax(Y_1, 1)\n",
    "correct_prediction_1 = tf.equal(pred_1, true_1)\n",
    "accuracy_1 = tf.reduce_mean(tf.cast(correct_prediction_1, tf.float32))\n",
    "\n",
    "\n",
    "pred_2 = tf.argmax(y_pred_2, 1)\n",
    "true_2 = tf.argmax(Y_2, 1)\n",
    "correct_prediction_2 = tf.equal(pred_2, true_2)\n",
    "accuracy_2 = tf.reduce_mean(tf.cast(correct_prediction_2, tf.float32))\n",
    "\n",
    "pred_3 = tf.argmax(y_pred_3, 1)\n",
    "true_3 = tf.argmax(Y_3, 1)\n",
    "correct_prediction_3 = tf.equal(pred_3, true_3)\n",
    "accuracy_3 = tf.reduce_mean(tf.cast(correct_prediction_3, tf.float32))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep:  1 Bc: 159 | train_1=0.234 test_1=0.164 | train_2=1.000 test_2=0.194 | train_3=1.000 test_3=0.180 | joint_loss=4.278\n",
      "Ep:  2 Bc: 159 | train_1=0.453 test_1=0.389 | train_2=0.734 test_2=0.590 | train_3=0.766 test_3=0.642 | joint_loss=4.183\n",
      "Ep:  3 Bc: 159 | train_1=0.453 test_1=0.447 | train_2=0.672 test_2=0.606 | train_3=0.734 test_3=0.661 | joint_loss=3.605\n",
      "Ep:  4 Bc: 159 | train_1=0.406 test_1=0.472 | train_2=0.562 test_2=0.599 | train_3=0.594 test_3=0.666 | joint_loss=3.416\n",
      "Ep:  5 Bc: 159 | train_1=0.531 test_1=0.489 | train_2=0.734 test_2=0.624 | train_3=0.766 test_3=0.667 | joint_loss=3.285\n",
      "Ep:  6 Bc: 159 | train_1=0.516 test_1=0.493 | train_2=0.703 test_2=0.629 | train_3=0.750 test_3=0.672 | joint_loss=3.219\n",
      "Ep:  7 Bc: 159 | train_1=0.406 test_1=0.519 | train_2=0.688 test_2=0.640 | train_3=0.719 test_3=0.679 | joint_loss=3.132\n",
      "Ep:  8 Bc: 159 | train_1=0.516 test_1=0.522 | train_2=0.656 test_2=0.635 | train_3=0.672 test_3=0.671 | joint_loss=3.043\n",
      "Ep:  9 Bc: 159 | train_1=0.531 test_1=0.528 | train_2=0.703 test_2=0.634 | train_3=0.781 test_3=0.686 | joint_loss=2.991\n",
      "Ep: 10 Bc: 159 | train_1=0.641 test_1=0.546 | train_2=0.578 test_2=0.658 | train_3=0.656 test_3=0.698 | joint_loss=2.914\n",
      "Ep: 11 Bc: 159 | train_1=0.516 test_1=0.553 | train_2=0.547 test_2=0.646 | train_3=0.531 test_3=0.686 | joint_loss=2.841\n",
      "Ep: 12 Bc: 159 | train_1=0.562 test_1=0.533 | train_2=0.688 test_2=0.648 | train_3=0.734 test_3=0.700 | joint_loss=2.770\n",
      "Ep: 13 Bc: 159 | train_1=0.562 test_1=0.550 | train_2=0.703 test_2=0.645 | train_3=0.766 test_3=0.699 | joint_loss=2.696\n",
      "Ep: 14 Bc: 159 | train_1=0.500 test_1=0.551 | train_2=0.703 test_2=0.664 | train_3=0.734 test_3=0.711 | joint_loss=2.608\n",
      "Ep: 15 Bc: 159 | train_1=0.625 test_1=0.555 | train_2=0.703 test_2=0.653 | train_3=0.781 test_3=0.702 | joint_loss=2.509\n",
      "Ep: 16 Bc: 159 | train_1=0.703 test_1=0.543 | train_2=0.672 test_2=0.637 | train_3=0.672 test_3=0.684 | joint_loss=2.400\n",
      "Ep: 17 Bc: 159 | train_1=0.609 test_1=0.569 | train_2=0.719 test_2=0.643 | train_3=0.766 test_3=0.694 | joint_loss=2.307\n",
      "Ep: 18 Bc: 159 | train_1=0.719 test_1=0.577 | train_2=0.812 test_2=0.643 | train_3=0.828 test_3=0.694 | joint_loss=2.191\n",
      "Ep: 19 Bc: 159 | train_1=0.781 test_1=0.574 | train_2=0.781 test_2=0.665 | train_3=0.859 test_3=0.703 | joint_loss=2.090\n",
      "Ep: 20 Bc: 159 | train_1=0.688 test_1=0.567 | train_2=0.750 test_2=0.645 | train_3=0.797 test_3=0.682 | joint_loss=1.979\n",
      "Ep: 21 Bc: 159 | train_1=0.609 test_1=0.578 | train_2=0.734 test_2=0.643 | train_3=0.828 test_3=0.692 | joint_loss=1.847\n",
      "Ep: 22 Bc: 159 | train_1=0.750 test_1=0.562 | train_2=0.828 test_2=0.636 | train_3=0.875 test_3=0.677 | joint_loss=1.792\n",
      "Ep: 23 Bc: 159 | train_1=0.812 test_1=0.574 | train_2=0.812 test_2=0.647 | train_3=0.812 test_3=0.685 | joint_loss=1.616\n",
      "Ep: 24 Bc: 159 | train_1=0.766 test_1=0.555 | train_2=0.828 test_2=0.643 | train_3=0.938 test_3=0.684 | joint_loss=1.525\n",
      "Ep: 25 Bc: 159 | train_1=0.812 test_1=0.548 | train_2=0.922 test_2=0.639 | train_3=0.953 test_3=0.690 | joint_loss=1.448\n",
      "Ep: 26 Bc: 159 | train_1=0.766 test_1=0.559 | train_2=0.969 test_2=0.651 | train_3=0.922 test_3=0.703 | joint_loss=1.335\n",
      "Ep: 27 Bc: 159 | train_1=0.672 test_1=0.566 | train_2=0.906 test_2=0.634 | train_3=0.938 test_3=0.688 | joint_loss=1.248\n",
      "Ep: 28 Bc: 159 | train_1=0.875 test_1=0.557 | train_2=0.922 test_2=0.655 | train_3=0.938 test_3=0.711 | joint_loss=1.214\n",
      "Ep: 29 Bc: 159 | train_1=0.812 test_1=0.557 | train_2=0.891 test_2=0.636 | train_3=0.938 test_3=0.681 | joint_loss=1.079\n",
      "Ep: 30 Bc: 159 | train_1=0.781 test_1=0.565 | train_2=0.953 test_2=0.636 | train_3=0.953 test_3=0.692 | joint_loss=0.977\n",
      "Ep: 31 Bc: 159 | train_1=0.844 test_1=0.546 | train_2=0.938 test_2=0.625 | train_3=0.969 test_3=0.669 | joint_loss=0.947\n",
      "Ep: 32 Bc: 159 | train_1=0.891 test_1=0.548 | train_2=0.922 test_2=0.622 | train_3=1.000 test_3=0.674 | joint_loss=0.885\n",
      "Ep: 33 Bc: 159 | train_1=0.906 test_1=0.558 | train_2=0.953 test_2=0.621 | train_3=0.969 test_3=0.662 | joint_loss=0.916\n",
      "Ep: 34 Bc: 159 | train_1=0.891 test_1=0.537 | train_2=0.938 test_2=0.614 | train_3=0.953 test_3=0.669 | joint_loss=0.768\n",
      "Ep: 35 Bc: 159 | train_1=0.938 test_1=0.544 | train_2=0.969 test_2=0.636 | train_3=0.922 test_3=0.678 | joint_loss=0.699\n",
      "Ep: 36 Bc: 159 | train_1=0.922 test_1=0.553 | train_2=0.984 test_2=0.633 | train_3=0.969 test_3=0.688 | joint_loss=0.724\n",
      "Ep: 37 Bc: 159 | train_1=0.891 test_1=0.544 | train_2=0.969 test_2=0.626 | train_3=0.984 test_3=0.675 | joint_loss=0.645\n",
      "Ep: 38 Bc: 159 | train_1=0.938 test_1=0.548 | train_2=0.938 test_2=0.630 | train_3=0.953 test_3=0.680 | joint_loss=0.588\n",
      "Ep: 39 Bc: 159 | train_1=0.906 test_1=0.557 | train_2=0.922 test_2=0.619 | train_3=0.938 test_3=0.683 | joint_loss=0.557\n",
      "Ep: 40 Bc: 159 | train_1=0.875 test_1=0.557 | train_2=0.984 test_2=0.614 | train_3=0.969 test_3=0.666 | joint_loss=0.623\n",
      "Process Time :1601.25 s\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "# Training cycle\n",
    "all_test_x = data.test.vectors_1\n",
    "all_test_x_pre = data.test.vectors_2\n",
    "all_test_y_1 = data.test.labels_1\n",
    "all_test_y_2 = data.test.labels_2\n",
    "all_test_y_3 = data.test.labels_3\n",
    "\n",
    "start = time.time()\n",
    "for epoch_i in range(training_epochs):\n",
    "    ave_cost = 0\n",
    "    for batch_i in range(total_batch):\n",
    "        batch_x, batch_x_pre, batch_y_1, batch_y_2, batch_y_3 = data.train.next_batch(batch_size, 3)\n",
    "        _, c = sess.run(\n",
    "            [optimizer_joint, joint_loss],\n",
    "            feed_dict={\n",
    "                X: batch_x,\n",
    "                X_pre: batch_x_pre,\n",
    "                Y_1: batch_y_1,\n",
    "                Y_2: batch_y_2,\n",
    "                Y_3: batch_y_3,\n",
    "                keep_prob: train_dropout\n",
    "            })\n",
    "        ave_cost += c / total_batch\n",
    "    # Display logs per epoch step\n",
    "    if epoch_i % 1 == 0:\n",
    "        train_acc_1 = sess.run(\n",
    "            accuracy_1,\n",
    "            feed_dict={\n",
    "                X: batch_x,\n",
    "                X_pre: batch_x_pre,\n",
    "                Y_1: batch_y_1,\n",
    "                keep_prob: test_dropout\n",
    "            })\n",
    "        train_acc_2 = sess.run(\n",
    "            accuracy_2,\n",
    "            feed_dict={\n",
    "                X: batch_x,\n",
    "                X_pre: batch_x_pre,\n",
    "                Y_2: batch_y_2,\n",
    "                keep_prob: test_dropout\n",
    "            })\n",
    "        train_acc_3 = sess.run(\n",
    "            accuracy_3,\n",
    "            feed_dict={\n",
    "                X: batch_x,\n",
    "                X_pre: batch_x_pre,\n",
    "                Y_3: batch_y_3,\n",
    "                keep_prob: test_dropout\n",
    "            })\n",
    "        \n",
    "        test_acc_1 = sess.run(\n",
    "            accuracy_1,\n",
    "            feed_dict={\n",
    "                X: all_test_x,\n",
    "                X_pre: all_test_x_pre,\n",
    "                Y_1: all_test_y_1,\n",
    "                keep_prob: test_dropout\n",
    "            })\n",
    "        test_acc_2 = sess.run(\n",
    "            accuracy_2,\n",
    "            feed_dict={\n",
    "                X: all_test_x,\n",
    "                X_pre: all_test_x_pre,\n",
    "                Y_2: all_test_y_2,\n",
    "                keep_prob: test_dropout\n",
    "            })\n",
    "        test_acc_3 = sess.run(\n",
    "            accuracy_3,\n",
    "            feed_dict={\n",
    "                X: all_test_x,\n",
    "                X_pre: all_test_x_pre,\n",
    "                Y_3: all_test_y_3,\n",
    "                keep_prob: test_dropout\n",
    "            })\n",
    "        print(\"Ep:%3d Bc:%4d\" % (epoch_i + 1, batch_i + 1),\n",
    "              \"| train_1=%.3f\" % train_acc_1, \"test_1=%.3f\" % test_acc_1, \n",
    "              \"| train_2=%.3f\" % train_acc_2, \"test_2=%.3f\" % test_acc_2, \n",
    "              \"| train_3=%.3f\" % train_acc_3, \"test_3=%.3f\" % test_acc_3, \"| joint_loss=%5.3f\" % ave_cost,)\n",
    "end = time.time()\n",
    "print(\"Process Time :%.2f s\" % (end - start))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
